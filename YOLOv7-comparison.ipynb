{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sSDOngglBk_O",
        "outputId": "9c63c795-5e74-445d-969b-c095d28de542"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade setuptools pip --user\n",
        "!pip install --ignore-installed PyYAML\n",
        "!pip install Pillow\n",
        "\n",
        "!pip install nvidia-pyindex\n",
        "!pip install --upgrade nvidia-tensorrt\n",
        "!pip install pycuda\n",
        "\n",
        "!pip install protobuf<4.21.3\n",
        "!pip install onnxruntime-gpu\n",
        "!pip install onnx>=1.9.0\n",
        "!pip install onnx-simplifier>=0.3.6 --user"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hQ5fNost-gZI",
        "outputId": "7be7a416-abd9-40f0-8166-6f1679edcd65"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Python version: 3.8.16 (default, Dec  7 2022, 01:12:13) \n",
            "[GCC 7.5.0], sys.version_info(major=3, minor=8, micro=16, releaselevel='final', serial=0) \n",
            "Pytorch version: 1.13.0+cu116 \n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import torch\n",
        "print(f\"Python version: {sys.version}, {sys.version_info} \")\n",
        "print(f\"Pytorch version: {torch.__version__} \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "feCaRUEI-_Os",
        "outputId": "131ec518-3de8-4906-fa8e-93f9e10ddf89"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fri Jan 13 13:47:58 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   38C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yfZALjuo-_Md",
        "outputId": "d4fab30d-77ba-4b94-c5b4-10c60a28871f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'yolov7'...\n",
            "remote: Enumerating objects: 1127, done.\u001b[K\n",
            "remote: Total 1127 (delta 0), reused 0 (delta 0), pack-reused 1127\u001b[K\n",
            "Receiving objects: 100% (1127/1127), 69.93 MiB | 16.27 MiB/s, done.\n",
            "Resolving deltas: 100% (525/525), done.\n",
            "/content/yolov7\n",
            "cfg\tdetect.py  hubconf.py  models\t  requirements.txt  tools\t  utils\n",
            "data\texport.py  inference   paper\t  scripts\t    train_aux.py\n",
            "deploy\tfigure\t   LICENSE.md  README.md  test.py\t    train.py\n"
          ]
        }
      ],
      "source": [
        "!# Download YOLOv7 code\n",
        "!git clone https://github.com/WongKinYiu/yolov7\n",
        "%cd yolov7\n",
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eWlHa1NJ-_Jw",
        "outputId": "3e8b98b7-c1d2-4ecd-80f1-6e0fc5071131"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2023-01-13 13:51:39--  https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7-tiny.pt\n",
            "Resolving github.com (github.com)... 20.205.243.166\n",
            "Connecting to github.com (github.com)|20.205.243.166|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/511187726/ba7d01ee-125a-4134-8864-fa1abcbf94d5?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20230113%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20230113T135139Z&X-Amz-Expires=300&X-Amz-Signature=a1f04ef4c809d81128baf1f58b571cfdf0fda6ce6aa4c89f198a2e3a28e494f8&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=511187726&response-content-disposition=attachment%3B%20filename%3Dyolov7-tiny.pt&response-content-type=application%2Foctet-stream [following]\n",
            "--2023-01-13 13:51:39--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/511187726/ba7d01ee-125a-4134-8864-fa1abcbf94d5?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20230113%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20230113T135139Z&X-Amz-Expires=300&X-Amz-Signature=a1f04ef4c809d81128baf1f58b571cfdf0fda6ce6aa4c89f198a2e3a28e494f8&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=511187726&response-content-disposition=attachment%3B%20filename%3Dyolov7-tiny.pt&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 12639769 (12M) [application/octet-stream]\n",
            "Saving to: â€˜yolov7-tiny.ptâ€™\n",
            "\n",
            "yolov7-tiny.pt      100%[===================>]  12.05M  7.52MB/s    in 1.6s    \n",
            "\n",
            "2023-01-13 13:51:42 (7.52 MB/s) - â€˜yolov7-tiny.ptâ€™ saved [12639769/12639769]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!# Download trained weights\n",
        "!wget https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7-tiny.pt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "daqrFRRjgBUL"
      },
      "outputs": [],
      "source": [
        "%cd /usr/src/tensorrt/samples/trtexec\n",
        "!make"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test YOLOv7 with NMS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YyRB8favmXys",
        "outputId": "9e073b92-158c-46f1-eec2-4b3b44b59745"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/yolov7\n",
            "Import onnx_graphsurgeon failure: No module named 'onnx_graphsurgeon'\n",
            "Namespace(batch_size=1, conf_thres=0.35, device='cpu', dynamic=False, dynamic_batch=False, end2end=True, fp16=False, grid=True, img_size=[640, 640], include_nms=False, int8=False, iou_thres=0.65, max_wh=None, simplify=True, topk_all=100, weights='./yolov7-tiny.pt')\n",
            "YOLOR ðŸš€ v0.1-121-g2fdc7f1 torch 1.13.0+cu116 CPU\n",
            "\n",
            "Fusing layers... \n",
            "Model Summary: 200 layers, 6219709 parameters, 6219709 gradients\n",
            "/usr/local/lib/python3.8/dist-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3190.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
            "\n",
            "Starting TorchScript export with torch 1.13.0+cu116...\n",
            "/content/yolov7/models/yolo.py:52: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  if self.grid[i].shape[2:4] != x[i].shape[2:4]:\n",
            "TorchScript export success, saved as ./yolov7-tiny.torchscript.pt\n",
            "CoreML export failure: No module named 'coremltools'\n",
            "\n",
            "Starting TorchScript-Lite export with torch 1.13.0+cu116...\n",
            "TorchScript-Lite export success, saved as ./yolov7-tiny.torchscript.ptl\n",
            "\n",
            "Starting ONNX export with onnx 1.13.0...\n",
            "\n",
            "Starting export end2end onnx model for TensorRT...\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:673: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at aten/src/ATen/core/TensorBody.h:480.)\n",
            "  if param.grad is not None:\n",
            "/content/yolov7/models/experimental.py:144: FutureWarning: 'torch.onnx._patch_torch._graph_op' is deprecated in version 1.13 and will be removed in version 1.14. Please note 'g.op()' is to be removed from torch.Graph. Please open a GitHub issue if you need this functionality..\n",
            "  out = g.op(\"TRT::EfficientNMS_TRT\",\n",
            "/usr/local/lib/python3.8/dist-packages/torch/onnx/_patch_torch.py:81: UserWarning: The shape inference of TRT::EfficientNMS_TRT type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (Triggered internally at ../torch/csrc/jit/passes/onnx/shape_type_inference.cpp:1884.)\n",
            "  _C._jit_pass_onnx_node_shape_type_inference(\n",
            "/usr/local/lib/python3.8/dist-packages/torch/onnx/utils.py:687: UserWarning: The shape inference of TRT::EfficientNMS_TRT type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (Triggered internally at ../torch/csrc/jit/passes/onnx/shape_type_inference.cpp:1884.)\n",
            "  _C._jit_pass_onnx_graph_shape_type_inference(\n",
            "/usr/local/lib/python3.8/dist-packages/torch/onnx/utils.py:1178: UserWarning: The shape inference of TRT::EfficientNMS_TRT type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (Triggered internally at ../torch/csrc/jit/passes/onnx/shape_type_inference.cpp:1884.)\n",
            "  _C._jit_pass_onnx_graph_shape_type_inference(\n",
            "\n",
            "Starting to simplify ONNX...\n",
            "ONNX export success, saved as ./yolov7-tiny.onnx\n",
            "\n",
            "Export complete (7.57s). Visualize with https://github.com/lutzroeder/netron.\n"
          ]
        }
      ],
      "source": [
        "# Save temp ONN model with NMS layer\n",
        "%cd /content/yolov7/\n",
        "!python export.py --weights ./yolov7-tiny.pt --grid --end2end --simplify --topk-all 100 --iou-thres 0.65 --conf-thres 0.35 --img-size 640 640"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t4clwTYTcbfl",
        "outputId": "6d91d4f1-162b-4960-8d13-74cd40bacf67"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "&&&& RUNNING TensorRT.trtexec [TensorRT v8502] # /usr/src/tensorrt/bin/trtexec --onnx=/content/yolov7/yolov7-tiny.onnx --saveEngine=/content/yolov7nms.trt --fp16\n",
            "[01/13/2023-15:16:52] [I] === Model Options ===\n",
            "[01/13/2023-15:16:52] [I] Format: ONNX\n",
            "[01/13/2023-15:16:52] [I] Model: /content/yolov7/yolov7-tiny.onnx\n",
            "[01/13/2023-15:16:52] [I] Output:\n",
            "[01/13/2023-15:16:52] [I] === Build Options ===\n",
            "[01/13/2023-15:16:52] [I] Max batch: explicit batch\n",
            "[01/13/2023-15:16:52] [I] Memory Pools: workspace: default, dlaSRAM: default, dlaLocalDRAM: default, dlaGlobalDRAM: default\n",
            "[01/13/2023-15:16:52] [I] minTiming: 1\n",
            "[01/13/2023-15:16:52] [I] avgTiming: 8\n",
            "[01/13/2023-15:16:52] [I] Precision: FP32+FP16\n",
            "[01/13/2023-15:16:52] [I] LayerPrecisions: \n",
            "[01/13/2023-15:16:52] [I] Calibration: \n",
            "[01/13/2023-15:16:52] [I] Refit: Disabled\n",
            "[01/13/2023-15:16:52] [I] Sparsity: Disabled\n",
            "[01/13/2023-15:16:52] [I] Safe mode: Disabled\n",
            "[01/13/2023-15:16:52] [I] DirectIO mode: Disabled\n",
            "[01/13/2023-15:16:52] [I] Restricted mode: Disabled\n",
            "[01/13/2023-15:16:52] [I] Build only: Disabled\n",
            "[01/13/2023-15:16:52] [I] Save engine: /content/yolov7nms.trt\n",
            "[01/13/2023-15:16:52] [I] Load engine: \n",
            "[01/13/2023-15:16:52] [I] Profiling verbosity: 0\n",
            "[01/13/2023-15:16:52] [I] Tactic sources: Using default tactic sources\n",
            "[01/13/2023-15:16:52] [I] timingCacheMode: local\n",
            "[01/13/2023-15:16:52] [I] timingCacheFile: \n",
            "[01/13/2023-15:16:52] [I] Heuristic: Disabled\n",
            "[01/13/2023-15:16:52] [I] Preview Features: Use default preview flags.\n",
            "[01/13/2023-15:16:52] [I] Input(s)s format: fp32:CHW\n",
            "[01/13/2023-15:16:52] [I] Output(s)s format: fp32:CHW\n",
            "[01/13/2023-15:16:52] [I] Input build shapes: model\n",
            "[01/13/2023-15:16:52] [I] Input calibration shapes: model\n",
            "[01/13/2023-15:16:52] [I] === System Options ===\n",
            "[01/13/2023-15:16:52] [I] Device: 0\n",
            "[01/13/2023-15:16:52] [I] DLACore: \n",
            "[01/13/2023-15:16:52] [I] Plugins:\n",
            "[01/13/2023-15:16:52] [I] === Inference Options ===\n",
            "[01/13/2023-15:16:52] [I] Batch: Explicit\n",
            "[01/13/2023-15:16:52] [I] Input inference shapes: model\n",
            "[01/13/2023-15:16:52] [I] Iterations: 10\n",
            "[01/13/2023-15:16:52] [I] Duration: 3s (+ 200ms warm up)\n",
            "[01/13/2023-15:16:52] [I] Sleep time: 0ms\n",
            "[01/13/2023-15:16:52] [I] Idle time: 0ms\n",
            "[01/13/2023-15:16:52] [I] Streams: 1\n",
            "[01/13/2023-15:16:52] [I] ExposeDMA: Disabled\n",
            "[01/13/2023-15:16:52] [I] Data transfers: Enabled\n",
            "[01/13/2023-15:16:52] [I] Spin-wait: Disabled\n",
            "[01/13/2023-15:16:52] [I] Multithreading: Disabled\n",
            "[01/13/2023-15:16:52] [I] CUDA Graph: Disabled\n",
            "[01/13/2023-15:16:52] [I] Separate profiling: Disabled\n",
            "[01/13/2023-15:16:52] [I] Time Deserialize: Disabled\n",
            "[01/13/2023-15:16:52] [I] Time Refit: Disabled\n",
            "[01/13/2023-15:16:52] [I] NVTX verbosity: 0\n",
            "[01/13/2023-15:16:52] [I] Persistent Cache Ratio: 0\n",
            "[01/13/2023-15:16:52] [I] Inputs:\n",
            "[01/13/2023-15:16:52] [I] === Reporting Options ===\n",
            "[01/13/2023-15:16:52] [I] Verbose: Disabled\n",
            "[01/13/2023-15:16:52] [I] Averages: 10 inferences\n",
            "[01/13/2023-15:16:52] [I] Percentiles: 90,95,99\n",
            "[01/13/2023-15:16:52] [I] Dump refittable layers:Disabled\n",
            "[01/13/2023-15:16:52] [I] Dump output: Disabled\n",
            "[01/13/2023-15:16:52] [I] Profile: Disabled\n",
            "[01/13/2023-15:16:52] [I] Export timing to JSON file: \n",
            "[01/13/2023-15:16:52] [I] Export output to JSON file: \n",
            "[01/13/2023-15:16:52] [I] Export profile to JSON file: \n",
            "[01/13/2023-15:16:52] [I] \n",
            "[01/13/2023-15:16:52] [I] === Device Information ===\n",
            "[01/13/2023-15:16:52] [I] Selected Device: Tesla T4\n",
            "[01/13/2023-15:16:52] [I] Compute Capability: 7.5\n",
            "[01/13/2023-15:16:52] [I] SMs: 40\n",
            "[01/13/2023-15:16:52] [I] Compute Clock Rate: 1.59 GHz\n",
            "[01/13/2023-15:16:52] [I] Device Global Memory: 15109 MiB\n",
            "[01/13/2023-15:16:52] [I] Shared Memory per SM: 64 KiB\n",
            "[01/13/2023-15:16:52] [I] Memory Bus Width: 256 bits (ECC enabled)\n",
            "[01/13/2023-15:16:52] [I] Memory Clock Rate: 5.001 GHz\n",
            "[01/13/2023-15:16:52] [I] \n",
            "[01/13/2023-15:16:52] [I] TensorRT version: 8.5.2\n",
            "[01/13/2023-15:16:53] [I] [TRT] [MemUsageChange] Init CUDA: CPU +0, GPU +0, now: CPU 0, GPU 1327 (MiB)\n",
            "[01/13/2023-15:16:57] [I] [TRT] [MemUsageChange] Init builder kernel library: CPU +0, GPU +74, now: CPU 0, GPU 1401 (MiB)\n",
            "[01/13/2023-15:16:57] [W] [TRT] CUDA lazy loading is not enabled. Enabling it can significantly reduce device memory usage. See `CUDA_MODULE_LOADING` in https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#env-vars\n",
            "[01/13/2023-15:16:57] [I] Start parsing network model\n",
            "[01/13/2023-15:16:57] [I] [TRT] ----------------------------------------------------------------\n",
            "[01/13/2023-15:16:57] [I] [TRT] Input filename:   /content/yolov7/yolov7-tiny.onnx\n",
            "[01/13/2023-15:16:57] [I] [TRT] ONNX IR version:  0.0.7\n",
            "[01/13/2023-15:16:57] [I] [TRT] Opset version:    12\n",
            "[01/13/2023-15:16:57] [I] [TRT] Producer name:    pytorch\n",
            "[01/13/2023-15:16:57] [I] [TRT] Producer version: 1.13.0\n",
            "[01/13/2023-15:16:57] [I] [TRT] Domain:           \n",
            "[01/13/2023-15:16:57] [I] [TRT] Model version:    0\n",
            "[01/13/2023-15:16:57] [I] [TRT] Doc string:       \n",
            "[01/13/2023-15:16:57] [I] [TRT] ----------------------------------------------------------------\n",
            "[01/13/2023-15:16:57] [W] [TRT] onnx2trt_utils.cpp:377: Your ONNX model has been generated with INT64 weights, while TensorRT does not natively support INT64. Attempting to cast down to INT32.\n",
            "[01/13/2023-15:16:57] [W] [TRT] onnx2trt_utils.cpp:403: One or more weights outside the range of INT32 was clamped\n",
            "[01/13/2023-15:16:57] [I] [TRT] No importer registered for op: EfficientNMS_TRT. Attempting to import as plugin.\n",
            "[01/13/2023-15:16:57] [I] [TRT] Searching for plugin: EfficientNMS_TRT, plugin_version: 1, plugin_namespace: \n",
            "[01/13/2023-15:16:57] [I] [TRT] Successfully created plugin: EfficientNMS_TRT\n",
            "[01/13/2023-15:16:57] [I] Finish parsing network model\n",
            "[01/13/2023-15:16:58] [I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +168, now: CPU 0, GPU 1569 (MiB)\n",
            "[01/13/2023-15:16:59] [I] [TRT] [MemUsageChange] Init cuDNN: CPU +0, GPU +172, now: CPU 0, GPU 1741 (MiB)\n",
            "[01/13/2023-15:16:59] [W] [TRT] TensorRT was linked against cuDNN 8.6.0 but loaded cuDNN 8.1.1\n",
            "[01/13/2023-15:16:59] [I] [TRT] Local timing cache in use. Profiling results in this builder pass will not be stored.\n",
            "[01/13/2023-15:25:33] [I] [TRT] Total Activation Memory: 15953719808\n",
            "[01/13/2023-15:25:33] [I] [TRT] Detected 1 inputs and 4 output network tensors.\n",
            "[01/13/2023-15:25:33] [I] [TRT] Total Host Persistent Memory: 167360\n",
            "[01/13/2023-15:25:33] [I] [TRT] Total Device Persistent Memory: 687616\n",
            "[01/13/2023-15:25:33] [I] [TRT] Total Scratch Memory: 40320768\n",
            "[01/13/2023-15:25:33] [I] [TRT] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 16 MiB, GPU 4371 MiB\n",
            "[01/13/2023-15:25:33] [I] [TRT] [BlockAssignment] Started assigning block shifts. This will take 110 steps to complete.\n",
            "[01/13/2023-15:25:33] [I] [TRT] [BlockAssignment] Algorithm ShiftNTopDown took 3.71246ms to assign 7 blocks to 110 nodes requiring 52276736 bytes.\n",
            "[01/13/2023-15:25:33] [I] [TRT] Total Activation Memory: 52276736\n",
            "[01/13/2023-15:25:34] [I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +8, now: CPU 0, GPU 1967 (MiB)\n",
            "[01/13/2023-15:25:34] [I] [TRT] [MemUsageChange] Init cuDNN: CPU +0, GPU +10, now: CPU 0, GPU 1977 (MiB)\n",
            "[01/13/2023-15:25:34] [W] [TRT] TensorRT was linked against cuDNN 8.6.0 but loaded cuDNN 8.1.1\n",
            "[01/13/2023-15:25:34] [W] [TRT] TensorRT encountered issues when converting weights between types and that could affect accuracy.\n",
            "[01/13/2023-15:25:34] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to adjust the magnitude of the weights.\n",
            "[01/13/2023-15:25:34] [W] [TRT] Check verbose logs for the list of affected weights.\n",
            "[01/13/2023-15:25:34] [W] [TRT] - 50 weights are affected by this issue: Detected subnormal FP16 values.\n",
            "[01/13/2023-15:25:34] [I] [TRT] [MemUsageChange] TensorRT-managed allocation in building engine: CPU +12, GPU +12, now: CPU 12, GPU 12 (MiB)\n",
            "[01/13/2023-15:25:34] [I] Engine built in 522 sec.\n",
            "[01/13/2023-15:25:34] [I] [TRT] Loaded engine size: 13 MiB\n",
            "[01/13/2023-15:25:34] [I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +10, now: CPU 0, GPU 1875 (MiB)\n",
            "[01/13/2023-15:25:34] [I] [TRT] [MemUsageChange] Init cuDNN: CPU +0, GPU +8, now: CPU 0, GPU 1883 (MiB)\n",
            "[01/13/2023-15:25:34] [W] [TRT] TensorRT was linked against cuDNN 8.6.0 but loaded cuDNN 8.1.1\n",
            "[01/13/2023-15:25:34] [I] [TRT] [MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +0, GPU +12, now: CPU 0, GPU 12 (MiB)\n",
            "[01/13/2023-15:25:34] [I] Engine deserialized in 0.0313738 sec.\n",
            "[01/13/2023-15:25:34] [I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +8, now: CPU 0, GPU 1875 (MiB)\n",
            "[01/13/2023-15:25:34] [I] [TRT] [MemUsageChange] Init cuDNN: CPU +0, GPU +8, now: CPU 0, GPU 1883 (MiB)\n",
            "[01/13/2023-15:25:34] [W] [TRT] TensorRT was linked against cuDNN 8.6.0 but loaded cuDNN 8.1.1\n",
            "[01/13/2023-15:25:34] [I] [TRT] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +51, now: CPU 0, GPU 63 (MiB)\n",
            "[01/13/2023-15:25:34] [W] [TRT] CUDA lazy loading is not enabled. Enabling it can significantly reduce device memory usage. See `CUDA_MODULE_LOADING` in https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#env-vars\n",
            "[01/13/2023-15:25:34] [I] Setting persistentCacheLimit to 0 bytes.\n",
            "[01/13/2023-15:25:34] [I] Using random values for input images\n",
            "[01/13/2023-15:25:34] [I] Created input binding for images with dimensions 1x3x640x640\n",
            "[01/13/2023-15:25:34] [I] Using random values for output num_dets\n",
            "[01/13/2023-15:25:34] [I] Created output binding for num_dets with dimensions 1x1\n",
            "[01/13/2023-15:25:34] [I] Using random values for output det_boxes\n",
            "[01/13/2023-15:25:34] [I] Created output binding for det_boxes with dimensions 1x100x4\n",
            "[01/13/2023-15:25:34] [I] Using random values for output det_scores\n",
            "[01/13/2023-15:25:34] [I] Created output binding for det_scores with dimensions 1x100\n",
            "[01/13/2023-15:25:34] [I] Using random values for output det_classes\n",
            "[01/13/2023-15:25:34] [I] Created output binding for det_classes with dimensions 1x100\n",
            "[01/13/2023-15:25:34] [I] Starting inference\n",
            "[01/13/2023-15:25:37] [I] Warmup completed 86 queries over 200 ms\n",
            "[01/13/2023-15:25:37] [I] Timing trace has 1365 queries over 3.00616 s\n",
            "[01/13/2023-15:25:37] [I] \n",
            "[01/13/2023-15:25:37] [I] === Trace details ===\n",
            "[01/13/2023-15:25:37] [I] Trace averages of 10 runs:\n",
            "[01/13/2023-15:25:37] [I] Average on 10 runs - GPU latency: 2.16739 ms - Host latency: 2.62309 ms (enqueue 0.805078 ms)\n",
            "[01/13/2023-15:25:37] [I] Average on 10 runs - GPU latency: 2.16848 ms - Host latency: 2.62516 ms (enqueue 0.804692 ms)\n",
            "[01/13/2023-15:25:37] [I] Average on 10 runs - GPU latency: 2.18295 ms - Host latency: 2.63593 ms (enqueue 0.800719 ms)\n",
            "[01/13/2023-15:25:37] [I] Average on 10 runs - GPU latency: 2.1819 ms - Host latency: 2.63394 ms (enqueue 0.782776 ms)\n",
            "[01/13/2023-15:25:37] [I] Average on 10 runs - GPU latency: 2.18069 ms - Host latency: 2.63224 ms (enqueue 0.819781 ms)\n",
            "[01/13/2023-15:25:37] [I] Average on 10 runs - GPU latency: 2.18312 ms - Host latency: 2.6355 ms (enqueue 0.814398 ms)\n",
            "[01/13/2023-15:25:37] [I] Average on 10 runs - GPU latency: 2.17365 ms - Host latency: 2.62768 ms (enqueue 0.852609 ms)\n",
            "[01/13/2023-15:25:37] [I] Average on 10 runs - GPU latency: 2.19014 ms - Host latency: 2.70396 ms (enqueue 0.831473 ms)\n",
            "[01/13/2023-15:25:37] [I] Average on 10 runs - GPU latency: 2.18915 ms - Host latency: 2.64478 ms (enqueue 0.808121 ms)\n",
            "[01/13/2023-15:25:37] [I] Average on 10 runs - GPU latency: 2.18247 ms - Host latency: 2.63823 ms (enqueue 0.822842 ms)\n",
            "[01/13/2023-15:25:37] [I] Average on 10 runs - GPU latency: 2.18091 ms - Host latency: 2.637 ms (enqueue 0.829004 ms)\n",
            "[01/13/2023-15:25:37] [I] Average on 10 runs - GPU latency: 2.18374 ms - Host latency: 2.63973 ms (enqueue 0.812515 ms)\n",
            "[01/13/2023-15:25:37] [I] Average on 10 runs - GPU latency: 2.18025 ms - Host latency: 2.6353 ms (enqueue 0.837216 ms)\n",
            "[01/13/2023-15:25:37] [I] Average on 10 runs - GPU latency: 2.15757 ms - Host latency: 2.61442 ms (enqueue 0.814203 ms)\n",
            "[01/13/2023-15:25:37] [I] Average on 10 runs - GPU latency: 2.15673 ms - Host latency: 2.60743 ms (enqueue 0.82518 ms)\n",
            "[01/13/2023-15:25:37] [I] Average on 10 runs - GPU latency: 2.16958 ms - Host latency: 2.61952 ms (enqueue 0.81333 ms)\n",
            "[01/13/2023-15:25:37] [I] Average on 10 runs - GPU latency: 2.20064 ms - Host latency: 2.65233 ms (enqueue 0.947345 ms)\n",
            "[01/13/2023-15:25:37] [I] Average on 10 runs - GPU latency: 2.19152 ms - Host latency: 2.66844 ms (enqueue 0.823767 ms)\n",
            "[01/13/2023-15:25:37] [I] Average on 10 runs - GPU latency: 2.18244 ms - Host latency: 2.63895 ms (enqueue 0.836261 ms)\n",
            "[01/13/2023-15:25:37] [I] Average on 10 runs - GPU latency: 2.16384 ms - Host latency: 2.62646 ms (enqueue 1.09117 ms)\n",
            "[01/13/2023-15:25:37] [I] Average on 10 runs - GPU latency: 2.14398 ms - Host latency: 2.59427 ms (enqueue 0.812366 ms)\n",
            "[01/13/2023-15:25:37] [I] Average on 10 runs - GPU latency: 2.14517 ms - Host latency: 2.6029 ms (enqueue 0.876306 ms)\n",
            "[01/13/2023-15:25:37] [I] Average on 10 runs - GPU latency: 2.1611 ms - Host latency: 2.61451 ms (enqueue 0.870032 ms)\n",
            "[01/13/2023-15:25:37] [I] Average on 10 runs - GPU latency: 2.16678 ms - Host latency: 2.65813 ms (enqueue 0.868256 ms)\n",
            "[01/13/2023-15:25:37] [I] Average on 10 runs - GPU latency: 2.16226 ms - Host latency: 2.64344 ms (enqueue 0.832202 ms)\n",
            "[01/13/2023-15:25:37] [I] Average on 10 runs - GPU latency: 2.16555 ms - Host latency: 2.6194 ms (enqueue 0.848669 ms)\n",
            "[01/13/2023-15:25:37] [I] Average on 10 runs - GPU latency: 2.1712 ms - Host latency: 2.66484 ms (enqueue 0.845825 ms)\n",
            "[01/13/2023-15:25:37] [I] Average on 10 runs - GPU latency: 2.18707 ms - Host latency: 2.64233 ms (enqueue 0.848267 ms)\n",
            "[01/13/2023-15:25:37] [I] Average on 10 runs - GPU latency: 2.18149 ms - Host latency: 2.63522 ms (enqueue 0.828705 ms)\n",
            "[01/13/2023-15:25:37] [I] Average on 10 runs - GPU latency: 2.18603 ms - Host latency: 2.64083 ms (enqueue 0.839709 ms)\n",
            "[01/13/2023-15:25:37] [I] Average on 10 runs - GPU latency: 2.1798 ms - Host latency: 2.63013 ms (enqueue 0.824316 ms)\n",
            "[01/13/2023-15:25:37] [I] Average on 10 runs - GPU latency: 2.16312 ms - Host latency: 2.61631 ms (enqueue 0.798688 ms)\n",
            "[01/13/2023-15:25:37] [I] Average on 10 runs - GPU latency: 2.15457 ms - Host latency: 2.60438 ms (enqueue 0.798907 ms)\n",
            "[01/13/2023-15:25:37] [I] Average on 10 runs - GPU latency: 2.19476 ms - Host latency: 2.64625 ms (enqueue 0.793909 ms)\n",
            "[01/13/2023-15:25:37] [I] Average on 10 runs - GPU latency: 2.20546 ms - Host latency: 2.6782 ms (enqueue 0.815991 ms)\n",
            "[01/13/2023-15:25:37] [I] Average on 10 runs - GPU latency: 2.20415 ms - Host latency: 2.65824 ms (enqueue 0.810938 ms)\n",
            "[01/13/2023-15:25:37] [I] Average on 10 runs - GPU latency: 2.22408 ms - Host latency: 2.67695 ms (enqueue 0.840088 ms)\n",
            "[01/13/2023-15:25:37] [I] Average on 10 runs - GPU latency: 2.24891 ms - Host latency: 2.7015 ms (enqueue 0.837653 ms)\n",
            "[01/13/2023-15:25:37] [I] Average on 10 runs - GPU latency: 2.26687 ms - Host latency: 2.72135 ms (enqueue 0.829285 ms)\n",
            "[01/13/2023-15:25:37] [I] Average on 10 runs - GPU latency: 2.26744 ms - Host latency: 2.72457 ms (enqueue 0.847009 ms)\n",
            "[01/13/2023-15:25:37] [I] Average on 10 runs - GPU latency: 2.26353 ms - Host latency: 2.71536 ms (enqueue 0.824219 ms)\n",
            "[01/13/2023-15:25:37] [I] Average on 10 runs - GPU latency: 2.24821 ms - Host latency: 2.70117 ms (enqueue 0.846033 ms)\n",
            "[01/13/2023-15:25:37] [I] Average on 10 runs - GPU latency: 2.2226 ms - Host latency: 2.67609 ms (enqueue 0.860901 ms)\n",
            "[01/13/2023-15:25:37] [I] Average on 10 runs - GPU latency: 2.20887 ms - Host latency: 2.70421 ms (enqueue 0.837146 ms)\n",
            "[01/13/2023-15:25:37] [I] Average on 10 runs - GPU latency: 2.19758 ms - Host latency: 2.65079 ms (enqueue 0.818933 ms)\n",
            "[01/13/2023-15:25:37] [I] Average on 10 runs - GPU latency: 2.19982 ms - Host latency: 2.65009 ms (enqueue 0.813367 ms)\n",
            "[01/13/2023-15:25:37] [I] Average on 10 runs - GPU latency: 2.19998 ms - Host latency: 2.65359 ms (enqueue 0.814441 ms)\n",
            "[01/13/2023-15:25:37] [I] Average on 10 runs - GPU latency: 2.1933 ms - Host latency: 2.6504 ms (enqueue 0.84314 ms)\n",
            "[01/13/2023-15:25:37] [I] Average on 10 runs - GPU latency: 2.19603 ms - Host latency: 2.65713 ms (enqueue 0.846887 ms)\n",
            "[01/13/2023-15:25:37] [I] Average on 10 runs - GPU latency: 2.22745 ms - Host latency: 2.72528 ms (enqueue 0.834338 ms)\n",
            "[01/13/2023-15:25:37] [I] Average on 10 runs - GPU latency: 2.20688 ms - Host latency: 2.66475 ms (enqueue 0.814355 ms)\n",
            "[01/13/2023-15:25:37] [I] Average on 10 runs - GPU latency: 2.18571 ms - Host latency: 2.64285 ms (enqueue 0.82478 ms)\n",
            "[01/13/2023-15:25:37] [I] Average on 10 runs - GPU latency: 2.1822 ms - Host latency: 2.6376 ms (enqueue 0.814697 ms)\n",
            "[01/13/2023-15:25:37] [I] Average on 10 runs - GPU latency: 2.19708 ms - Host latency: 2.65167 ms (enqueue 0.795093 ms)\n",
            "[01/13/2023-15:25:37] [I] Average on 10 runs - GPU latency: 2.20035 ms - Host latency: 2.6559 ms (enqueue 0.801147 ms)\n",
            "[01/13/2023-15:25:37] [I] Average on 10 runs - GPU latency: 2.18173 ms - Host latency: 2.63621 ms (enqueue 0.78623 ms)\n",
            "[01/13/2023-15:25:37] [I] Average on 10 runs - GPU latency: 2.18052 ms - Host latency: 2.63446 ms (enqueue 0.814514 ms)\n",
            "[01/13/2023-15:25:37] [I] Average on 10 runs - GPU latency: 2.1807 ms - Host latency: 2.63363 ms (enqueue 0.80907 ms)\n",
            "[01/13/2023-15:25:37] [I] Average on 10 runs - GPU latency: 2.18391 ms - Host latency: 2.63767 ms (enqueue 0.809766 ms)\n",
            "[01/13/2023-15:25:37] [I] Average on 10 runs - GPU latency: 2.17272 ms - Host latency: 2.63326 ms (enqueue 0.809912 ms)\n",
            "[01/13/2023-15:25:37] [I] Average on 10 runs - GPU latency: 2.17933 ms - Host latency: 2.65343 ms (enqueue 0.842517 ms)\n",
            "[01/13/2023-15:25:37] [I] Average on 10 runs - GPU latency: 2.20085 ms - Host latency: 2.65649 ms (enqueue 0.787695 ms)\n",
            "[01/13/2023-15:25:37] [I] Average on 10 runs - GPU latency: 2.1984 ms - Host latency: 2.65398 ms (enqueue 0.896533 ms)\n",
            "[01/13/2023-15:25:37] [I] Average on 10 runs - GPU latency: 2.1993 ms - Host latency: 2.65659 ms (enqueue 0.795178 ms)\n",
            "[01/13/2023-15:25:37] [I] Average on 10 runs - GPU latency: 2.18385 ms - Host latency: 2.63811 ms (enqueue 0.819739 ms)\n",
            "[01/13/2023-15:25:37] [I] Average on 10 runs - GPU latency: 2.18038 ms - Host latency: 2.63557 ms (enqueue 0.989014 ms)\n",
            "[01/13/2023-15:25:37] [I] Average on 10 runs - GPU latency: 2.18529 ms - Host latency: 2.64384 ms (enqueue 0.840881 ms)\n",
            "[01/13/2023-15:25:37] [I] Average on 10 runs - GPU latency: 2.18585 ms - Host latency: 2.67643 ms (enqueue 0.893921 ms)\n",
            "[01/13/2023-15:25:37] [I] Average on 10 runs - GPU latency: 2.17739 ms - Host latency: 2.63402 ms (enqueue 0.836511 ms)\n",
            "[01/13/2023-15:25:37] [I] Average on 10 runs - GPU latency: 2.15754 ms - Host latency: 2.61255 ms (enqueue 0.839001 ms)\n",
            "[01/13/2023-15:25:37] [I] Average on 10 runs - GPU latency: 2.1708 ms - Host latency: 2.62637 ms (enqueue 0.834033 ms)\n",
            "[01/13/2023-15:25:37] [I] Average on 10 runs - GPU latency: 2.20161 ms - Host latency: 2.65896 ms (enqueue 0.828345 ms)\n",
            "[01/13/2023-15:25:37] [I] Average on 10 runs - GPU latency: 2.19979 ms - Host latency: 2.65654 ms (enqueue 0.81427 ms)\n",
            "[01/13/2023-15:25:37] [I] Average on 10 runs - GPU latency: 2.18616 ms - Host latency: 2.64155 ms (enqueue 0.844885 ms)\n",
            "[01/13/2023-15:25:37] [I] Average on 10 runs - GPU latency: 2.18064 ms - Host latency: 2.63778 ms (enqueue 0.828186 ms)\n",
            "[01/13/2023-15:25:37] [I] Average on 10 runs - GPU latency: 2.17989 ms - Host latency: 2.63516 ms (enqueue 0.821069 ms)\n",
            "[01/13/2023-15:25:37] [I] Average on 10 runs - GPU latency: 2.1811 ms - Host latency: 2.63547 ms (enqueue 0.805493 ms)\n",
            "[01/13/2023-15:25:37] [I] Average on 10 runs - GPU latency: 2.18159 ms - Host latency: 2.6385 ms (enqueue 0.817541 ms)\n",
            "[01/13/2023-15:25:37] [I] Average on 10 runs - GPU latency: 2.16089 ms - Host latency: 2.63014 ms (enqueue 0.821314 ms)\n",
            "[01/13/2023-15:25:37] [I] Average on 10 runs - GPU latency: 2.19988 ms - Host latency: 2.65334 ms (enqueue 0.849976 ms)\n",
            "[01/13/2023-15:25:37] [I] Average on 10 runs - GPU latency: 2.20016 ms - Host latency: 2.65221 ms (enqueue 0.804712 ms)\n",
            "[01/13/2023-15:25:37] [I] Average on 10 runs - GPU latency: 2.20608 ms - Host latency: 2.66355 ms (enqueue 0.819714 ms)\n",
            "[01/13/2023-15:25:37] [I] Average on 10 runs - GPU latency: 2.22015 ms - Host latency: 2.67505 ms (enqueue 0.833435 ms)\n",
            "[01/13/2023-15:25:37] [I] Average on 10 runs - GPU latency: 2.22181 ms - Host latency: 2.71724 ms (enqueue 0.821252 ms)\n",
            "[01/13/2023-15:25:37] [I] Average on 10 runs - GPU latency: 2.22267 ms - Host latency: 2.68085 ms (enqueue 0.849646 ms)\n",
            "[01/13/2023-15:25:37] [I] Average on 10 runs - GPU latency: 2.22048 ms - Host latency: 2.68171 ms (enqueue 0.831665 ms)\n",
            "[01/13/2023-15:25:37] [I] Average on 10 runs - GPU latency: 2.20457 ms - Host latency: 2.66055 ms (enqueue 0.835352 ms)\n",
            "[01/13/2023-15:25:37] [I] Average on 10 runs - GPU latency: 2.23472 ms - Host latency: 2.6947 ms (enqueue 0.846069 ms)\n",
            "[01/13/2023-15:25:37] [I] Average on 10 runs - GPU latency: 2.24094 ms - Host latency: 2.70293 ms (enqueue 0.851221 ms)\n",
            "[01/13/2023-15:25:37] [I] Average on 10 runs - GPU latency: 2.21685 ms - Host latency: 2.67207 ms (enqueue 0.879956 ms)\n",
            "[01/13/2023-15:25:37] [I] Average on 10 runs - GPU latency: 2.20098 ms - Host latency: 2.65395 ms (enqueue 0.834375 ms)\n",
            "[01/13/2023-15:25:37] [I] Average on 10 runs - GPU latency: 2.20107 ms - Host latency: 2.65491 ms (enqueue 0.844604 ms)\n",
            "[01/13/2023-15:25:37] [I] Average on 10 runs - GPU latency: 2.19905 ms - Host latency: 2.65303 ms (enqueue 0.851392 ms)\n",
            "[01/13/2023-15:25:37] [I] Average on 10 runs - GPU latency: 2.20105 ms - Host latency: 2.68779 ms (enqueue 0.856885 ms)\n",
            "[01/13/2023-15:25:37] [I] Average on 10 runs - GPU latency: 2.19187 ms - Host latency: 2.64568 ms (enqueue 0.856494 ms)\n",
            "[01/13/2023-15:25:37] [I] Average on 10 runs - GPU latency: 2.20474 ms - Host latency: 2.68372 ms (enqueue 0.86792 ms)\n",
            "[01/13/2023-15:25:37] [I] Average on 10 runs - GPU latency: 2.21931 ms - Host latency: 2.67476 ms (enqueue 0.868311 ms)\n",
            "[01/13/2023-15:25:37] [I] Average on 10 runs - GPU latency: 2.20815 ms - Host latency: 2.70593 ms (enqueue 0.936377 ms)\n",
            "[01/13/2023-15:25:37] [I] Average on 10 runs - GPU latency: 2.18447 ms - Host latency: 2.66853 ms (enqueue 0.858765 ms)\n",
            "[01/13/2023-15:25:37] [I] Average on 10 runs - GPU latency: 2.18564 ms - Host latency: 2.70344 ms (enqueue 0.817407 ms)\n",
            "[01/13/2023-15:25:37] [I] Average on 10 runs - GPU latency: 2.18022 ms - Host latency: 2.63896 ms (enqueue 0.826953 ms)\n",
            "[01/13/2023-15:25:37] [I] Average on 10 runs - GPU latency: 2.18474 ms - Host latency: 2.6489 ms (enqueue 0.803516 ms)\n",
            "[01/13/2023-15:25:37] [I] Average on 10 runs - GPU latency: 2.17856 ms - Host latency: 2.63494 ms (enqueue 0.816406 ms)\n",
            "[01/13/2023-15:25:37] [I] Average on 10 runs - GPU latency: 2.17344 ms - Host latency: 2.62883 ms (enqueue 0.806763 ms)\n",
            "[01/13/2023-15:25:37] [I] Average on 10 runs - GPU latency: 2.20232 ms - Host latency: 2.65955 ms (enqueue 0.813892 ms)\n",
            "[01/13/2023-15:25:37] [I] Average on 10 runs - GPU latency: 2.20024 ms - Host latency: 2.67051 ms (enqueue 0.845117 ms)\n",
            "[01/13/2023-15:25:37] [I] Average on 10 runs - GPU latency: 2.19795 ms - Host latency: 2.65322 ms (enqueue 0.800684 ms)\n",
            "[01/13/2023-15:25:37] [I] Average on 10 runs - GPU latency: 2.20315 ms - Host latency: 2.69219 ms (enqueue 0.815723 ms)\n",
            "[01/13/2023-15:25:37] [I] Average on 10 runs - GPU latency: 2.19001 ms - Host latency: 2.64497 ms (enqueue 1.00715 ms)\n",
            "[01/13/2023-15:25:37] [I] Average on 10 runs - GPU latency: 2.18411 ms - Host latency: 2.65435 ms (enqueue 0.821338 ms)\n",
            "[01/13/2023-15:25:37] [I] Average on 10 runs - GPU latency: 2.18093 ms - Host latency: 2.64072 ms (enqueue 0.8073 ms)\n",
            "[01/13/2023-15:25:37] [I] Average on 10 runs - GPU latency: 2.18333 ms - Host latency: 2.65398 ms (enqueue 0.919238 ms)\n",
            "[01/13/2023-15:25:37] [I] Average on 10 runs - GPU latency: 2.17874 ms - Host latency: 2.63259 ms (enqueue 0.995581 ms)\n",
            "[01/13/2023-15:25:37] [I] Average on 10 runs - GPU latency: 2.1679 ms - Host latency: 2.6179 ms (enqueue 0.806812 ms)\n",
            "[01/13/2023-15:25:37] [I] Average on 10 runs - GPU latency: 2.20168 ms - Host latency: 2.65508 ms (enqueue 0.805566 ms)\n",
            "[01/13/2023-15:25:37] [I] Average on 10 runs - GPU latency: 2.20032 ms - Host latency: 2.65168 ms (enqueue 0.811353 ms)\n",
            "[01/13/2023-15:25:37] [I] Average on 10 runs - GPU latency: 2.20491 ms - Host latency: 2.65933 ms (enqueue 0.814722 ms)\n",
            "[01/13/2023-15:25:37] [I] Average on 10 runs - GPU latency: 2.20361 ms - Host latency: 2.66528 ms (enqueue 0.806616 ms)\n",
            "[01/13/2023-15:25:37] [I] Average on 10 runs - GPU latency: 2.18684 ms - Host latency: 2.64175 ms (enqueue 0.813232 ms)\n",
            "[01/13/2023-15:25:37] [I] Average on 10 runs - GPU latency: 2.18071 ms - Host latency: 2.6325 ms (enqueue 0.789062 ms)\n",
            "[01/13/2023-15:25:37] [I] Average on 10 runs - GPU latency: 2.18198 ms - Host latency: 2.63752 ms (enqueue 0.796631 ms)\n",
            "[01/13/2023-15:25:37] [I] Average on 10 runs - GPU latency: 2.18201 ms - Host latency: 2.63762 ms (enqueue 0.819092 ms)\n",
            "[01/13/2023-15:25:37] [I] Average on 10 runs - GPU latency: 2.18411 ms - Host latency: 2.64368 ms (enqueue 0.80979 ms)\n",
            "[01/13/2023-15:25:37] [I] Average on 10 runs - GPU latency: 2.16177 ms - Host latency: 2.61765 ms (enqueue 0.830322 ms)\n",
            "[01/13/2023-15:25:37] [I] Average on 10 runs - GPU latency: 2.20681 ms - Host latency: 2.66638 ms (enqueue 0.839502 ms)\n",
            "[01/13/2023-15:25:37] [I] Average on 10 runs - GPU latency: 2.2209 ms - Host latency: 2.68076 ms (enqueue 0.929395 ms)\n",
            "[01/13/2023-15:25:37] [I] Average on 10 runs - GPU latency: 2.21936 ms - Host latency: 2.67344 ms (enqueue 0.944653 ms)\n",
            "[01/13/2023-15:25:37] [I] Average on 10 runs - GPU latency: 2.22424 ms - Host latency: 2.67786 ms (enqueue 0.78728 ms)\n",
            "[01/13/2023-15:25:37] [I] Average on 10 runs - GPU latency: 2.21919 ms - Host latency: 2.6717 ms (enqueue 0.806714 ms)\n",
            "[01/13/2023-15:25:37] [I] Average on 10 runs - GPU latency: 2.20632 ms - Host latency: 2.65984 ms (enqueue 0.817139 ms)\n",
            "[01/13/2023-15:25:37] [I] Average on 10 runs - GPU latency: 2.23223 ms - Host latency: 2.7175 ms (enqueue 0.827271 ms)\n",
            "[01/13/2023-15:25:37] [I] Average on 10 runs - GPU latency: 2.24077 ms - Host latency: 2.69458 ms (enqueue 0.839648 ms)\n",
            "[01/13/2023-15:25:37] [I] Average on 10 runs - GPU latency: 2.24451 ms - Host latency: 2.69717 ms (enqueue 0.868604 ms)\n",
            "[01/13/2023-15:25:37] [I] Average on 10 runs - GPU latency: 2.23838 ms - Host latency: 2.72139 ms (enqueue 0.796289 ms)\n",
            "[01/13/2023-15:25:37] [I] Average on 10 runs - GPU latency: 2.21975 ms - Host latency: 2.67249 ms (enqueue 0.82749 ms)\n",
            "[01/13/2023-15:25:37] [I] Average on 10 runs - GPU latency: 2.22083 ms - Host latency: 2.67493 ms (enqueue 0.812988 ms)\n",
            "[01/13/2023-15:25:37] [I] \n",
            "[01/13/2023-15:25:37] [I] === Performance summary ===\n",
            "[01/13/2023-15:25:37] [I] Throughput: 454.068 qps\n",
            "[01/13/2023-15:25:37] [I] Latency: min = 2.586 ms, max = 3.06982 ms, mean = 2.65487 ms, median = 2.64832 ms, percentile(90%) = 2.69092 ms, percentile(95%) = 2.71265 ms, percentile(99%) = 2.91187 ms\n",
            "[01/13/2023-15:25:37] [I] Enqueue Time: min = 0.742737 ms, max = 2.13525 ms, mean = 0.836159 ms, median = 0.820801 ms, percentile(90%) = 0.889648 ms, percentile(95%) = 0.944824 ms, percentile(99%) = 1.17621 ms\n",
            "[01/13/2023-15:25:37] [I] H2D Latency: min = 0.428986 ms, max = 0.830933 ms, mean = 0.447395 ms, median = 0.440918 ms, percentile(90%) = 0.451172 ms, percentile(95%) = 0.45752 ms, percentile(99%) = 0.71167 ms\n",
            "[01/13/2023-15:25:37] [I] GPU Compute Time: min = 2.13483 ms, max = 2.28271 ms, mean = 2.19449 ms, median = 2.19238 ms, percentile(90%) = 2.22607 ms, percentile(95%) = 2.24268 ms, percentile(99%) = 2.26709 ms\n",
            "[01/13/2023-15:25:37] [I] D2H Latency: min = 0.00830078 ms, max = 0.0268555 ms, mean = 0.0129875 ms, median = 0.0129395 ms, percentile(90%) = 0.013916 ms, percentile(95%) = 0.0144043 ms, percentile(99%) = 0.0195312 ms\n",
            "[01/13/2023-15:25:37] [I] Total Host Walltime: 3.00616 s\n",
            "[01/13/2023-15:25:37] [I] Total GPU Compute Time: 2.99548 s\n",
            "[01/13/2023-15:25:37] [W] * GPU compute time is unstable, with coefficient of variance = 1.16569%.\n",
            "[01/13/2023-15:25:37] [W]   If not already in use, locking GPU clock frequency or adding --useSpinWait may improve the stability.\n",
            "[01/13/2023-15:25:37] [I] Explanations of the performance metrics are printed in the verbose logs.\n",
            "[01/13/2023-15:25:37] [I] \n",
            "&&&& PASSED TensorRT.trtexec [TensorRT v8502] # /usr/src/tensorrt/bin/trtexec --onnx=/content/yolov7/yolov7-tiny.onnx --saveEngine=/content/yolov7nms.trt --fp16\n"
          ]
        }
      ],
      "source": [
        "# Create engine\n",
        "!/usr/src/tensorrt/bin/trtexec --onnx=/content/yolov7/yolov7-tiny.onnx --saveEngine=/content/yolov7nms.trt --fp16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HnhF0aH1vUJC",
        "outputId": "0078dc79-c33e-410c-b152-1f88d64348f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "&&&& RUNNING TensorRT.trtexec [TensorRT v8502] # /usr/src/tensorrt/bin/trtexec --loadEngine=/content/yolov7nms.trt --batch=1 --warmUp=500 --duration=1 --iterations=500 --streams=1 --useSpinWait\n",
            "[01/13/2023-15:26:36] [I] === Model Options ===\n",
            "[01/13/2023-15:26:36] [I] Format: *\n",
            "[01/13/2023-15:26:36] [I] Model: \n",
            "[01/13/2023-15:26:36] [I] Output:\n",
            "[01/13/2023-15:26:36] [I] === Build Options ===\n",
            "[01/13/2023-15:26:36] [I] Max batch: 1\n",
            "[01/13/2023-15:26:36] [I] Memory Pools: workspace: default, dlaSRAM: default, dlaLocalDRAM: default, dlaGlobalDRAM: default\n",
            "[01/13/2023-15:26:36] [I] minTiming: 1\n",
            "[01/13/2023-15:26:36] [I] avgTiming: 8\n",
            "[01/13/2023-15:26:36] [I] Precision: FP32\n",
            "[01/13/2023-15:26:36] [I] LayerPrecisions: \n",
            "[01/13/2023-15:26:36] [I] Calibration: \n",
            "[01/13/2023-15:26:36] [I] Refit: Disabled\n",
            "[01/13/2023-15:26:36] [I] Sparsity: Disabled\n",
            "[01/13/2023-15:26:36] [I] Safe mode: Disabled\n",
            "[01/13/2023-15:26:36] [I] DirectIO mode: Disabled\n",
            "[01/13/2023-15:26:36] [I] Restricted mode: Disabled\n",
            "[01/13/2023-15:26:36] [I] Build only: Disabled\n",
            "[01/13/2023-15:26:36] [I] Save engine: \n",
            "[01/13/2023-15:26:36] [I] Load engine: /content/yolov7nms.trt\n",
            "[01/13/2023-15:26:36] [I] Profiling verbosity: 0\n",
            "[01/13/2023-15:26:36] [I] Tactic sources: Using default tactic sources\n",
            "[01/13/2023-15:26:36] [I] timingCacheMode: local\n",
            "[01/13/2023-15:26:36] [I] timingCacheFile: \n",
            "[01/13/2023-15:26:36] [I] Heuristic: Disabled\n",
            "[01/13/2023-15:26:36] [I] Preview Features: Use default preview flags.\n",
            "[01/13/2023-15:26:36] [I] Input(s)s format: fp32:CHW\n",
            "[01/13/2023-15:26:36] [I] Output(s)s format: fp32:CHW\n",
            "[01/13/2023-15:26:36] [I] Input build shapes: model\n",
            "[01/13/2023-15:26:36] [I] Input calibration shapes: model\n",
            "[01/13/2023-15:26:36] [I] === System Options ===\n",
            "[01/13/2023-15:26:36] [I] Device: 0\n",
            "[01/13/2023-15:26:36] [I] DLACore: \n",
            "[01/13/2023-15:26:36] [I] Plugins:\n",
            "[01/13/2023-15:26:36] [I] === Inference Options ===\n",
            "[01/13/2023-15:26:36] [I] Batch: 1\n",
            "[01/13/2023-15:26:36] [I] Input inference shapes: model\n",
            "[01/13/2023-15:26:36] [I] Iterations: 500\n",
            "[01/13/2023-15:26:36] [I] Duration: 1s (+ 500ms warm up)\n",
            "[01/13/2023-15:26:36] [I] Sleep time: 0ms\n",
            "[01/13/2023-15:26:36] [I] Idle time: 0ms\n",
            "[01/13/2023-15:26:36] [I] Streams: 1\n",
            "[01/13/2023-15:26:36] [I] ExposeDMA: Disabled\n",
            "[01/13/2023-15:26:36] [I] Data transfers: Enabled\n",
            "[01/13/2023-15:26:36] [I] Spin-wait: Enabled\n",
            "[01/13/2023-15:26:36] [I] Multithreading: Disabled\n",
            "[01/13/2023-15:26:36] [I] CUDA Graph: Disabled\n",
            "[01/13/2023-15:26:36] [I] Separate profiling: Disabled\n",
            "[01/13/2023-15:26:36] [I] Time Deserialize: Disabled\n",
            "[01/13/2023-15:26:36] [I] Time Refit: Disabled\n",
            "[01/13/2023-15:26:36] [I] NVTX verbosity: 0\n",
            "[01/13/2023-15:26:36] [I] Persistent Cache Ratio: 0\n",
            "[01/13/2023-15:26:36] [I] Inputs:\n",
            "[01/13/2023-15:26:36] [I] === Reporting Options ===\n",
            "[01/13/2023-15:26:36] [I] Verbose: Disabled\n",
            "[01/13/2023-15:26:36] [I] Averages: 10 inferences\n",
            "[01/13/2023-15:26:36] [I] Percentiles: 90,95,99\n",
            "[01/13/2023-15:26:36] [I] Dump refittable layers:Disabled\n",
            "[01/13/2023-15:26:36] [I] Dump output: Disabled\n",
            "[01/13/2023-15:26:36] [I] Profile: Disabled\n",
            "[01/13/2023-15:26:36] [I] Export timing to JSON file: \n",
            "[01/13/2023-15:26:36] [I] Export output to JSON file: \n",
            "[01/13/2023-15:26:36] [I] Export profile to JSON file: \n",
            "[01/13/2023-15:26:36] [I] \n",
            "[01/13/2023-15:26:36] [I] === Device Information ===\n",
            "[01/13/2023-15:26:36] [I] Selected Device: Tesla T4\n",
            "[01/13/2023-15:26:36] [I] Compute Capability: 7.5\n",
            "[01/13/2023-15:26:36] [I] SMs: 40\n",
            "[01/13/2023-15:26:36] [I] Compute Clock Rate: 1.59 GHz\n",
            "[01/13/2023-15:26:36] [I] Device Global Memory: 15109 MiB\n",
            "[01/13/2023-15:26:36] [I] Shared Memory per SM: 64 KiB\n",
            "[01/13/2023-15:26:36] [I] Memory Bus Width: 256 bits (ECC enabled)\n",
            "[01/13/2023-15:26:36] [I] Memory Clock Rate: 5.001 GHz\n",
            "[01/13/2023-15:26:36] [I] \n",
            "[01/13/2023-15:26:36] [I] TensorRT version: 8.5.2\n",
            "[01/13/2023-15:26:36] [I] Engine loaded in 0.0230077 sec.\n",
            "[01/13/2023-15:26:37] [I] [TRT] Loaded engine size: 13 MiB\n",
            "[01/13/2023-15:26:38] [I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +168, now: CPU 0, GPU 1507 (MiB)\n",
            "[01/13/2023-15:26:39] [I] [TRT] [MemUsageChange] Init cuDNN: CPU +0, GPU +172, now: CPU 0, GPU 1679 (MiB)\n",
            "[01/13/2023-15:26:39] [W] [TRT] TensorRT was linked against cuDNN 8.6.0 but loaded cuDNN 8.1.1\n",
            "[01/13/2023-15:26:39] [I] [TRT] [MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +0, GPU +12, now: CPU 0, GPU 12 (MiB)\n",
            "[01/13/2023-15:26:39] [I] Engine deserialized in 2.6418 sec.\n",
            "[01/13/2023-15:26:39] [I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +8, now: CPU 0, GPU 1671 (MiB)\n",
            "[01/13/2023-15:26:39] [I] [TRT] [MemUsageChange] Init cuDNN: CPU +0, GPU +8, now: CPU 0, GPU 1679 (MiB)\n",
            "[01/13/2023-15:26:39] [W] [TRT] TensorRT was linked against cuDNN 8.6.0 but loaded cuDNN 8.1.1\n",
            "[01/13/2023-15:26:39] [I] [TRT] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +51, now: CPU 0, GPU 63 (MiB)\n",
            "[01/13/2023-15:26:39] [W] [TRT] CUDA lazy loading is not enabled. Enabling it can significantly reduce device memory usage. See `CUDA_MODULE_LOADING` in https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#env-vars\n",
            "[01/13/2023-15:26:39] [I] Setting persistentCacheLimit to 0 bytes.\n",
            "[01/13/2023-15:26:39] [I] Using random values for input images\n",
            "[01/13/2023-15:26:39] [I] Created input binding for images with dimensions 1x3x640x640\n",
            "[01/13/2023-15:26:39] [I] Using random values for output num_dets\n",
            "[01/13/2023-15:26:39] [I] Created output binding for num_dets with dimensions 1x1\n",
            "[01/13/2023-15:26:39] [I] Using random values for output det_boxes\n",
            "[01/13/2023-15:26:39] [I] Created output binding for det_boxes with dimensions 1x100x4\n",
            "[01/13/2023-15:26:39] [I] Using random values for output det_scores\n",
            "[01/13/2023-15:26:39] [I] Created output binding for det_scores with dimensions 1x100\n",
            "[01/13/2023-15:26:39] [I] Using random values for output det_classes\n",
            "[01/13/2023-15:26:39] [I] Created output binding for det_classes with dimensions 1x100\n",
            "[01/13/2023-15:26:39] [I] Starting inference\n",
            "[01/13/2023-15:26:40] [I] Warmup completed 156 queries over 500 ms\n",
            "[01/13/2023-15:26:40] [I] Timing trace has 500 queries over 1.10238 s\n",
            "[01/13/2023-15:26:40] [I] \n",
            "[01/13/2023-15:26:40] [I] === Trace details ===\n",
            "[01/13/2023-15:26:40] [I] Trace averages of 10 runs:\n",
            "[01/13/2023-15:26:40] [I] Average on 10 runs - GPU latency: 2.18623 ms - Host latency: 2.64238 ms (enqueue 0.714612 ms)\n",
            "[01/13/2023-15:26:40] [I] Average on 10 runs - GPU latency: 2.18735 ms - Host latency: 2.64576 ms (enqueue 0.69621 ms)\n",
            "[01/13/2023-15:26:40] [I] Average on 10 runs - GPU latency: 2.18781 ms - Host latency: 2.6432 ms (enqueue 0.69729 ms)\n",
            "[01/13/2023-15:26:40] [I] Average on 10 runs - GPU latency: 2.17998 ms - Host latency: 2.63895 ms (enqueue 0.734552 ms)\n",
            "[01/13/2023-15:26:40] [I] Average on 10 runs - GPU latency: 2.17118 ms - Host latency: 2.62867 ms (enqueue 0.732294 ms)\n",
            "[01/13/2023-15:26:40] [I] Average on 10 runs - GPU latency: 2.16807 ms - Host latency: 2.62342 ms (enqueue 0.716089 ms)\n",
            "[01/13/2023-15:26:40] [I] Average on 10 runs - GPU latency: 2.17416 ms - Host latency: 2.62992 ms (enqueue 0.714697 ms)\n",
            "[01/13/2023-15:26:40] [I] Average on 10 runs - GPU latency: 2.16742 ms - Host latency: 2.62516 ms (enqueue 0.708191 ms)\n",
            "[01/13/2023-15:26:40] [I] Average on 10 runs - GPU latency: 2.17993 ms - Host latency: 2.63434 ms (enqueue 0.708124 ms)\n",
            "[01/13/2023-15:26:40] [I] Average on 10 runs - GPU latency: 2.18475 ms - Host latency: 2.63987 ms (enqueue 0.670905 ms)\n",
            "[01/13/2023-15:26:40] [I] Average on 10 runs - GPU latency: 2.18611 ms - Host latency: 2.64278 ms (enqueue 0.657568 ms)\n",
            "[01/13/2023-15:26:40] [I] Average on 10 runs - GPU latency: 2.18543 ms - Host latency: 2.64099 ms (enqueue 0.666437 ms)\n",
            "[01/13/2023-15:26:40] [I] Average on 10 runs - GPU latency: 2.18627 ms - Host latency: 2.63995 ms (enqueue 0.655164 ms)\n",
            "[01/13/2023-15:26:40] [I] Average on 10 runs - GPU latency: 2.16426 ms - Host latency: 2.61796 ms (enqueue 0.656976 ms)\n",
            "[01/13/2023-15:26:40] [I] Average on 10 runs - GPU latency: 2.16108 ms - Host latency: 2.61712 ms (enqueue 0.659393 ms)\n",
            "[01/13/2023-15:26:40] [I] Average on 10 runs - GPU latency: 2.15824 ms - Host latency: 2.61483 ms (enqueue 0.906079 ms)\n",
            "[01/13/2023-15:26:40] [I] Average on 10 runs - GPU latency: 2.16255 ms - Host latency: 2.61753 ms (enqueue 0.662103 ms)\n",
            "[01/13/2023-15:26:40] [I] Average on 10 runs - GPU latency: 2.16194 ms - Host latency: 2.61785 ms (enqueue 0.686749 ms)\n",
            "[01/13/2023-15:26:40] [I] Average on 10 runs - GPU latency: 2.18312 ms - Host latency: 2.6376 ms (enqueue 0.776569 ms)\n",
            "[01/13/2023-15:26:40] [I] Average on 10 runs - GPU latency: 2.20333 ms - Host latency: 2.66129 ms (enqueue 0.84502 ms)\n",
            "[01/13/2023-15:26:40] [I] Average on 10 runs - GPU latency: 2.20728 ms - Host latency: 2.66768 ms (enqueue 0.687256 ms)\n",
            "[01/13/2023-15:26:40] [I] Average on 10 runs - GPU latency: 2.19376 ms - Host latency: 2.65133 ms (enqueue 0.711774 ms)\n",
            "[01/13/2023-15:26:40] [I] Average on 10 runs - GPU latency: 2.19315 ms - Host latency: 2.68265 ms (enqueue 0.699585 ms)\n",
            "[01/13/2023-15:26:40] [I] Average on 10 runs - GPU latency: 2.18856 ms - Host latency: 2.64516 ms (enqueue 0.712183 ms)\n",
            "[01/13/2023-15:26:40] [I] Average on 10 runs - GPU latency: 2.19052 ms - Host latency: 2.64704 ms (enqueue 0.70672 ms)\n",
            "[01/13/2023-15:26:40] [I] Average on 10 runs - GPU latency: 2.18616 ms - Host latency: 2.67769 ms (enqueue 0.700879 ms)\n",
            "[01/13/2023-15:26:40] [I] Average on 10 runs - GPU latency: 2.16228 ms - Host latency: 2.61959 ms (enqueue 0.704834 ms)\n",
            "[01/13/2023-15:26:40] [I] Average on 10 runs - GPU latency: 2.16222 ms - Host latency: 2.62239 ms (enqueue 0.694311 ms)\n",
            "[01/13/2023-15:26:40] [I] Average on 10 runs - GPU latency: 2.16101 ms - Host latency: 2.61548 ms (enqueue 0.683276 ms)\n",
            "[01/13/2023-15:26:40] [I] Average on 10 runs - GPU latency: 2.1608 ms - Host latency: 2.61553 ms (enqueue 0.684351 ms)\n",
            "[01/13/2023-15:26:40] [I] Average on 10 runs - GPU latency: 2.19487 ms - Host latency: 2.65253 ms (enqueue 0.690881 ms)\n",
            "[01/13/2023-15:26:40] [I] Average on 10 runs - GPU latency: 2.25603 ms - Host latency: 2.71299 ms (enqueue 0.713123 ms)\n",
            "[01/13/2023-15:26:40] [I] Average on 10 runs - GPU latency: 2.3129 ms - Host latency: 2.77018 ms (enqueue 0.702649 ms)\n",
            "[01/13/2023-15:26:40] [I] Average on 10 runs - GPU latency: 2.33503 ms - Host latency: 2.83158 ms (enqueue 0.69436 ms)\n",
            "[01/13/2023-15:26:40] [I] Average on 10 runs - GPU latency: 2.33312 ms - Host latency: 2.79349 ms (enqueue 0.709985 ms)\n",
            "[01/13/2023-15:26:40] [I] Average on 10 runs - GPU latency: 2.2952 ms - Host latency: 2.75533 ms (enqueue 0.809338 ms)\n",
            "[01/13/2023-15:26:40] [I] Average on 10 runs - GPU latency: 2.26399 ms - Host latency: 2.71807 ms (enqueue 0.758484 ms)\n",
            "[01/13/2023-15:26:40] [I] Average on 10 runs - GPU latency: 2.23938 ms - Host latency: 2.69261 ms (enqueue 0.757153 ms)\n",
            "[01/13/2023-15:26:40] [I] Average on 10 runs - GPU latency: 2.21589 ms - Host latency: 2.66898 ms (enqueue 0.738379 ms)\n",
            "[01/13/2023-15:26:40] [I] Average on 10 runs - GPU latency: 2.19133 ms - Host latency: 2.6465 ms (enqueue 0.775232 ms)\n",
            "[01/13/2023-15:26:40] [I] Average on 10 runs - GPU latency: 2.19098 ms - Host latency: 2.68375 ms (enqueue 0.739099 ms)\n",
            "[01/13/2023-15:26:40] [I] Average on 10 runs - GPU latency: 2.18765 ms - Host latency: 2.64841 ms (enqueue 0.749683 ms)\n",
            "[01/13/2023-15:26:40] [I] Average on 10 runs - GPU latency: 2.18573 ms - Host latency: 2.6458 ms (enqueue 0.794763 ms)\n",
            "[01/13/2023-15:26:40] [I] Average on 10 runs - GPU latency: 2.19069 ms - Host latency: 2.68553 ms (enqueue 0.909753 ms)\n",
            "[01/13/2023-15:26:40] [I] Average on 10 runs - GPU latency: 2.16619 ms - Host latency: 2.62587 ms (enqueue 0.716431 ms)\n",
            "[01/13/2023-15:26:40] [I] Average on 10 runs - GPU latency: 2.20814 ms - Host latency: 2.66367 ms (enqueue 0.803589 ms)\n",
            "[01/13/2023-15:26:40] [I] Average on 10 runs - GPU latency: 2.20579 ms - Host latency: 2.66171 ms (enqueue 0.780347 ms)\n",
            "[01/13/2023-15:26:40] [I] Average on 10 runs - GPU latency: 2.20544 ms - Host latency: 2.66384 ms (enqueue 0.819336 ms)\n",
            "[01/13/2023-15:26:40] [I] Average on 10 runs - GPU latency: 2.20233 ms - Host latency: 2.6621 ms (enqueue 0.758301 ms)\n",
            "[01/13/2023-15:26:40] [I] Average on 10 runs - GPU latency: 2.18619 ms - Host latency: 2.64786 ms (enqueue 0.71864 ms)\n",
            "[01/13/2023-15:26:40] [I] \n",
            "[01/13/2023-15:26:40] [I] === Performance summary ===\n",
            "[01/13/2023-15:26:40] [I] Throughput: 453.562 qps\n",
            "[01/13/2023-15:26:40] [I] Latency: min = 2.59985 ms, max = 3.16846 ms, mean = 2.6587 ms, median = 2.64386 ms, percentile(90%) = 2.72327 ms, percentile(95%) = 2.7832 ms, percentile(99%) = 2.96631 ms\n",
            "[01/13/2023-15:26:40] [I] Enqueue Time: min = 0.616699 ms, max = 1.31885 ms, mean = 0.727794 ms, median = 0.705078 ms, percentile(90%) = 0.810181 ms, percentile(95%) = 0.893921 ms, percentile(99%) = 1.17511 ms\n",
            "[01/13/2023-15:26:40] [I] H2D Latency: min = 0.436401 ms, max = 0.817993 ms, mean = 0.449507 ms, median = 0.444519 ms, percentile(90%) = 0.4552 ms, percentile(95%) = 0.459106 ms, percentile(99%) = 0.76062 ms\n",
            "[01/13/2023-15:26:40] [I] GPU Compute Time: min = 2.15039 ms, max = 2.35913 ms, mean = 2.19824 ms, median = 2.18726 ms, percentile(90%) = 2.26306 ms, percentile(95%) = 2.32336 ms, percentile(99%) = 2.34033 ms\n",
            "[01/13/2023-15:26:40] [I] D2H Latency: min = 0.0078125 ms, max = 0.02771 ms, mean = 0.010955 ms, median = 0.0108032 ms, percentile(90%) = 0.0119629 ms, percentile(95%) = 0.0125122 ms, percentile(99%) = 0.0168457 ms\n",
            "[01/13/2023-15:26:40] [I] Total Host Walltime: 1.10238 s\n",
            "[01/13/2023-15:26:40] [I] Total GPU Compute Time: 1.09912 s\n",
            "[01/13/2023-15:26:40] [W] * GPU compute time is unstable, with coefficient of variance = 1.97745%.\n",
            "[01/13/2023-15:26:40] [W]   If not already in use, locking GPU clock frequency or adding --useSpinWait may improve the stability.\n",
            "[01/13/2023-15:26:40] [I] Explanations of the performance metrics are printed in the verbose logs.\n",
            "[01/13/2023-15:26:40] [I] \n",
            "&&&& PASSED TensorRT.trtexec [TensorRT v8502] # /usr/src/tensorrt/bin/trtexec --loadEngine=/content/yolov7nms.trt --batch=1 --warmUp=500 --duration=1 --iterations=500 --streams=1 --useSpinWait\n"
          ]
        }
      ],
      "source": [
        "# Run speed test\n",
        "!/usr/src/tensorrt/bin/trtexec --loadEngine=/content/yolov7nms.trt --batch=1 --warmUp=500 --duration=1 --iterations=500 --streams=1 --useSpinWait"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test YOLOv7 without NMS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gp6_ZrG5tMwZ"
      },
      "outputs": [],
      "source": [
        "# Save temp ONN model without NMS layer\n",
        "%cd /content/yolov7/\n",
        "!python export.py --weights ./yolov7-tiny.pt --grid --simplify --topk-all 100 --iou-thres 0.65 --conf-thres 0.35 --img-size 640 640"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N6Ckyme3nGut",
        "outputId": "de1baefe-ba43-4506-883e-e86fcf2f17b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "&&&& RUNNING TensorRT.trtexec [TensorRT v8502] # /usr/src/tensorrt/bin/trtexec --onnx=/content/yolov7/yolov7-tiny.onnx --saveEngine=/content/yolov7.trt --fp16\n",
            "[01/13/2023-15:34:06] [I] === Model Options ===\n",
            "[01/13/2023-15:34:06] [I] Format: ONNX\n",
            "[01/13/2023-15:34:06] [I] Model: /content/yolov7/yolov7-tiny.onnx\n",
            "[01/13/2023-15:34:06] [I] Output:\n",
            "[01/13/2023-15:34:06] [I] === Build Options ===\n",
            "[01/13/2023-15:34:06] [I] Max batch: explicit batch\n",
            "[01/13/2023-15:34:06] [I] Memory Pools: workspace: default, dlaSRAM: default, dlaLocalDRAM: default, dlaGlobalDRAM: default\n",
            "[01/13/2023-15:34:06] [I] minTiming: 1\n",
            "[01/13/2023-15:34:06] [I] avgTiming: 8\n",
            "[01/13/2023-15:34:06] [I] Precision: FP32+FP16\n",
            "[01/13/2023-15:34:06] [I] LayerPrecisions: \n",
            "[01/13/2023-15:34:06] [I] Calibration: \n",
            "[01/13/2023-15:34:06] [I] Refit: Disabled\n",
            "[01/13/2023-15:34:06] [I] Sparsity: Disabled\n",
            "[01/13/2023-15:34:06] [I] Safe mode: Disabled\n",
            "[01/13/2023-15:34:06] [I] DirectIO mode: Disabled\n",
            "[01/13/2023-15:34:06] [I] Restricted mode: Disabled\n",
            "[01/13/2023-15:34:06] [I] Build only: Disabled\n",
            "[01/13/2023-15:34:06] [I] Save engine: /content/yolov7.trt\n",
            "[01/13/2023-15:34:06] [I] Load engine: \n",
            "[01/13/2023-15:34:06] [I] Profiling verbosity: 0\n",
            "[01/13/2023-15:34:06] [I] Tactic sources: Using default tactic sources\n",
            "[01/13/2023-15:34:06] [I] timingCacheMode: local\n",
            "[01/13/2023-15:34:06] [I] timingCacheFile: \n",
            "[01/13/2023-15:34:06] [I] Heuristic: Disabled\n",
            "[01/13/2023-15:34:06] [I] Preview Features: Use default preview flags.\n",
            "[01/13/2023-15:34:06] [I] Input(s)s format: fp32:CHW\n",
            "[01/13/2023-15:34:06] [I] Output(s)s format: fp32:CHW\n",
            "[01/13/2023-15:34:06] [I] Input build shapes: model\n",
            "[01/13/2023-15:34:06] [I] Input calibration shapes: model\n",
            "[01/13/2023-15:34:06] [I] === System Options ===\n",
            "[01/13/2023-15:34:06] [I] Device: 0\n",
            "[01/13/2023-15:34:06] [I] DLACore: \n",
            "[01/13/2023-15:34:06] [I] Plugins:\n",
            "[01/13/2023-15:34:06] [I] === Inference Options ===\n",
            "[01/13/2023-15:34:06] [I] Batch: Explicit\n",
            "[01/13/2023-15:34:06] [I] Input inference shapes: model\n",
            "[01/13/2023-15:34:06] [I] Iterations: 10\n",
            "[01/13/2023-15:34:06] [I] Duration: 3s (+ 200ms warm up)\n",
            "[01/13/2023-15:34:06] [I] Sleep time: 0ms\n",
            "[01/13/2023-15:34:06] [I] Idle time: 0ms\n",
            "[01/13/2023-15:34:06] [I] Streams: 1\n",
            "[01/13/2023-15:34:06] [I] ExposeDMA: Disabled\n",
            "[01/13/2023-15:34:06] [I] Data transfers: Enabled\n",
            "[01/13/2023-15:34:06] [I] Spin-wait: Disabled\n",
            "[01/13/2023-15:34:06] [I] Multithreading: Disabled\n",
            "[01/13/2023-15:34:06] [I] CUDA Graph: Disabled\n",
            "[01/13/2023-15:34:06] [I] Separate profiling: Disabled\n",
            "[01/13/2023-15:34:06] [I] Time Deserialize: Disabled\n",
            "[01/13/2023-15:34:06] [I] Time Refit: Disabled\n",
            "[01/13/2023-15:34:06] [I] NVTX verbosity: 0\n",
            "[01/13/2023-15:34:06] [I] Persistent Cache Ratio: 0\n",
            "[01/13/2023-15:34:06] [I] Inputs:\n",
            "[01/13/2023-15:34:06] [I] === Reporting Options ===\n",
            "[01/13/2023-15:34:06] [I] Verbose: Disabled\n",
            "[01/13/2023-15:34:06] [I] Averages: 10 inferences\n",
            "[01/13/2023-15:34:06] [I] Percentiles: 90,95,99\n",
            "[01/13/2023-15:34:06] [I] Dump refittable layers:Disabled\n",
            "[01/13/2023-15:34:06] [I] Dump output: Disabled\n",
            "[01/13/2023-15:34:06] [I] Profile: Disabled\n",
            "[01/13/2023-15:34:06] [I] Export timing to JSON file: \n",
            "[01/13/2023-15:34:06] [I] Export output to JSON file: \n",
            "[01/13/2023-15:34:06] [I] Export profile to JSON file: \n",
            "[01/13/2023-15:34:06] [I] \n",
            "[01/13/2023-15:34:06] [I] === Device Information ===\n",
            "[01/13/2023-15:34:06] [I] Selected Device: Tesla T4\n",
            "[01/13/2023-15:34:06] [I] Compute Capability: 7.5\n",
            "[01/13/2023-15:34:06] [I] SMs: 40\n",
            "[01/13/2023-15:34:06] [I] Compute Clock Rate: 1.59 GHz\n",
            "[01/13/2023-15:34:06] [I] Device Global Memory: 15109 MiB\n",
            "[01/13/2023-15:34:06] [I] Shared Memory per SM: 64 KiB\n",
            "[01/13/2023-15:34:06] [I] Memory Bus Width: 256 bits (ECC enabled)\n",
            "[01/13/2023-15:34:06] [I] Memory Clock Rate: 5.001 GHz\n",
            "[01/13/2023-15:34:06] [I] \n",
            "[01/13/2023-15:34:06] [I] TensorRT version: 8.5.2\n",
            "[01/13/2023-15:34:06] [I] [TRT] [MemUsageChange] Init CUDA: CPU +0, GPU +0, now: CPU 0, GPU 1327 (MiB)\n",
            "[01/13/2023-15:34:10] [I] [TRT] [MemUsageChange] Init builder kernel library: CPU +0, GPU +74, now: CPU 0, GPU 1401 (MiB)\n",
            "[01/13/2023-15:34:10] [W] [TRT] CUDA lazy loading is not enabled. Enabling it can significantly reduce device memory usage. See `CUDA_MODULE_LOADING` in https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#env-vars\n",
            "[01/13/2023-15:34:10] [I] Start parsing network model\n",
            "[01/13/2023-15:34:10] [I] [TRT] ----------------------------------------------------------------\n",
            "[01/13/2023-15:34:10] [I] [TRT] Input filename:   /content/yolov7/yolov7-tiny.onnx\n",
            "[01/13/2023-15:34:10] [I] [TRT] ONNX IR version:  0.0.7\n",
            "[01/13/2023-15:34:10] [I] [TRT] Opset version:    12\n",
            "[01/13/2023-15:34:10] [I] [TRT] Producer name:    pytorch\n",
            "[01/13/2023-15:34:10] [I] [TRT] Producer version: 1.13.0\n",
            "[01/13/2023-15:34:10] [I] [TRT] Domain:           \n",
            "[01/13/2023-15:34:10] [I] [TRT] Model version:    0\n",
            "[01/13/2023-15:34:10] [I] [TRT] Doc string:       \n",
            "[01/13/2023-15:34:10] [I] [TRT] ----------------------------------------------------------------\n",
            "[01/13/2023-15:34:10] [W] [TRT] onnx2trt_utils.cpp:377: Your ONNX model has been generated with INT64 weights, while TensorRT does not natively support INT64. Attempting to cast down to INT32.\n",
            "[01/13/2023-15:34:10] [I] Finish parsing network model\n",
            "[01/13/2023-15:34:12] [I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +168, now: CPU 0, GPU 1569 (MiB)\n",
            "[01/13/2023-15:34:12] [I] [TRT] [MemUsageChange] Init cuDNN: CPU +0, GPU +172, now: CPU 0, GPU 1741 (MiB)\n",
            "[01/13/2023-15:34:12] [W] [TRT] TensorRT was linked against cuDNN 8.6.0 but loaded cuDNN 8.1.1\n",
            "[01/13/2023-15:34:12] [I] [TRT] Local timing cache in use. Profiling results in this builder pass will not be stored.\n",
            "[01/13/2023-15:42:22] [I] [TRT] Total Activation Memory: 15942496768\n",
            "[01/13/2023-15:42:22] [I] [TRT] Detected 1 inputs and 4 output network tensors.\n",
            "[01/13/2023-15:42:23] [I] [TRT] Total Host Persistent Memory: 158112\n",
            "[01/13/2023-15:42:23] [I] [TRT] Total Device Persistent Memory: 658432\n",
            "[01/13/2023-15:42:23] [I] [TRT] Total Scratch Memory: 3264000\n",
            "[01/13/2023-15:42:23] [I] [TRT] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 16 MiB, GPU 4371 MiB\n",
            "[01/13/2023-15:42:23] [I] [TRT] [BlockAssignment] Started assigning block shifts. This will take 102 steps to complete.\n",
            "[01/13/2023-15:42:23] [I] [TRT] [BlockAssignment] Algorithm ShiftNTopDown took 3.34746ms to assign 7 blocks to 102 nodes requiring 18713600 bytes.\n",
            "[01/13/2023-15:42:23] [I] [TRT] Total Activation Memory: 18713600\n",
            "[01/13/2023-15:42:23] [W] [TRT] TensorRT encountered issues when converting weights between types and that could affect accuracy.\n",
            "[01/13/2023-15:42:23] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to adjust the magnitude of the weights.\n",
            "[01/13/2023-15:42:23] [W] [TRT] Check verbose logs for the list of affected weights.\n",
            "[01/13/2023-15:42:23] [W] [TRT] - 50 weights are affected by this issue: Detected subnormal FP16 values.\n",
            "[01/13/2023-15:42:23] [I] [TRT] [MemUsageChange] TensorRT-managed allocation in building engine: CPU +12, GPU +12, now: CPU 12, GPU 12 (MiB)\n",
            "[01/13/2023-15:42:23] [I] Engine built in 497.717 sec.\n",
            "[01/13/2023-15:42:23] [I] [TRT] Loaded engine size: 13 MiB\n",
            "[01/13/2023-15:42:23] [I] [TRT] [MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +0, GPU +12, now: CPU 0, GPU 12 (MiB)\n",
            "[01/13/2023-15:42:23] [I] Engine deserialized in 0.0274127 sec.\n",
            "[01/13/2023-15:42:23] [I] [TRT] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +19, now: CPU 0, GPU 31 (MiB)\n",
            "[01/13/2023-15:42:23] [W] [TRT] CUDA lazy loading is not enabled. Enabling it can significantly reduce device memory usage. See `CUDA_MODULE_LOADING` in https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#env-vars\n",
            "[01/13/2023-15:42:23] [I] Setting persistentCacheLimit to 0 bytes.\n",
            "[01/13/2023-15:42:23] [I] Using random values for input images\n",
            "[01/13/2023-15:42:23] [I] Created input binding for images with dimensions 1x3x640x640\n",
            "[01/13/2023-15:42:23] [I] Using random values for output output\n",
            "[01/13/2023-15:42:23] [I] Created output binding for output with dimensions 1x25200x85\n",
            "[01/13/2023-15:42:23] [I] Starting inference\n",
            "[01/13/2023-15:42:27] [I] Warmup completed 95 queries over 200 ms\n",
            "[01/13/2023-15:42:27] [I] Timing trace has 1389 queries over 3.00684 s\n",
            "[01/13/2023-15:42:27] [I] \n",
            "[01/13/2023-15:42:27] [I] === Trace details ===\n",
            "[01/13/2023-15:42:27] [I] Trace averages of 10 runs:\n",
            "[01/13/2023-15:42:27] [I] Average on 10 runs - GPU latency: 2.14423 ms - Host latency: 3.30359 ms (enqueue 0.791954 ms)\n",
            "[01/13/2023-15:42:27] [I] Average on 10 runs - GPU latency: 2.15005 ms - Host latency: 3.31566 ms (enqueue 0.828462 ms)\n",
            "[01/13/2023-15:42:27] [I] Average on 10 runs - GPU latency: 2.14858 ms - Host latency: 3.31175 ms (enqueue 0.80793 ms)\n",
            "[01/13/2023-15:42:27] [I] Average on 10 runs - GPU latency: 2.14926 ms - Host latency: 3.33517 ms (enqueue 0.8198 ms)\n",
            "[01/13/2023-15:42:27] [I] Average on 10 runs - GPU latency: 2.12376 ms - Host latency: 3.28954 ms (enqueue 0.878656 ms)\n",
            "[01/13/2023-15:42:27] [I] Average on 10 runs - GPU latency: 2.11684 ms - Host latency: 3.279 ms (enqueue 0.810602 ms)\n",
            "[01/13/2023-15:42:27] [I] Average on 10 runs - GPU latency: 2.13452 ms - Host latency: 3.33894 ms (enqueue 0.844193 ms)\n",
            "[01/13/2023-15:42:27] [I] Average on 10 runs - GPU latency: 2.12088 ms - Host latency: 3.30278 ms (enqueue 0.942551 ms)\n",
            "[01/13/2023-15:42:27] [I] Average on 10 runs - GPU latency: 2.13147 ms - Host latency: 3.32469 ms (enqueue 0.809317 ms)\n",
            "[01/13/2023-15:42:27] [I] Average on 10 runs - GPU latency: 2.13195 ms - Host latency: 3.29263 ms (enqueue 0.810657 ms)\n",
            "[01/13/2023-15:42:27] [I] Average on 10 runs - GPU latency: 2.12073 ms - Host latency: 3.28455 ms (enqueue 0.818692 ms)\n",
            "[01/13/2023-15:42:27] [I] Average on 10 runs - GPU latency: 2.13227 ms - Host latency: 3.28589 ms (enqueue 0.812479 ms)\n",
            "[01/13/2023-15:42:27] [I] Average on 10 runs - GPU latency: 2.15341 ms - Host latency: 3.37022 ms (enqueue 0.900119 ms)\n",
            "[01/13/2023-15:42:27] [I] Average on 10 runs - GPU latency: 2.15556 ms - Host latency: 3.3603 ms (enqueue 0.786859 ms)\n",
            "[01/13/2023-15:42:27] [I] Average on 10 runs - GPU latency: 2.14227 ms - Host latency: 3.30218 ms (enqueue 0.788358 ms)\n",
            "[01/13/2023-15:42:27] [I] Average on 10 runs - GPU latency: 2.14792 ms - Host latency: 3.32203 ms (enqueue 0.796991 ms)\n",
            "[01/13/2023-15:42:27] [I] Average on 10 runs - GPU latency: 2.129 ms - Host latency: 3.29464 ms (enqueue 0.790857 ms)\n",
            "[01/13/2023-15:42:27] [I] Average on 10 runs - GPU latency: 2.12775 ms - Host latency: 3.30806 ms (enqueue 0.782532 ms)\n",
            "[01/13/2023-15:42:27] [I] Average on 10 runs - GPU latency: 2.12047 ms - Host latency: 3.28655 ms (enqueue 0.833636 ms)\n",
            "[01/13/2023-15:42:27] [I] Average on 10 runs - GPU latency: 2.1532 ms - Host latency: 3.34974 ms (enqueue 1.15582 ms)\n",
            "[01/13/2023-15:42:27] [I] Average on 10 runs - GPU latency: 2.13019 ms - Host latency: 3.30865 ms (enqueue 1.11501 ms)\n",
            "[01/13/2023-15:42:27] [I] Average on 10 runs - GPU latency: 2.12982 ms - Host latency: 3.29788 ms (enqueue 0.825977 ms)\n",
            "[01/13/2023-15:42:27] [I] Average on 10 runs - GPU latency: 2.13109 ms - Host latency: 3.29855 ms (enqueue 0.819177 ms)\n",
            "[01/13/2023-15:42:27] [I] Average on 10 runs - GPU latency: 2.13239 ms - Host latency: 3.29498 ms (enqueue 0.799365 ms)\n",
            "[01/13/2023-15:42:27] [I] Average on 10 runs - GPU latency: 2.12202 ms - Host latency: 3.28992 ms (enqueue 0.787476 ms)\n",
            "[01/13/2023-15:42:27] [I] Average on 10 runs - GPU latency: 2.14272 ms - Host latency: 3.30543 ms (enqueue 0.80304 ms)\n",
            "[01/13/2023-15:42:27] [I] Average on 10 runs - GPU latency: 2.16223 ms - Host latency: 3.32169 ms (enqueue 0.803705 ms)\n",
            "[01/13/2023-15:42:27] [I] Average on 10 runs - GPU latency: 2.16576 ms - Host latency: 3.32368 ms (enqueue 1.00413 ms)\n",
            "[01/13/2023-15:42:27] [I] Average on 10 runs - GPU latency: 2.16374 ms - Host latency: 3.31906 ms (enqueue 0.772791 ms)\n",
            "[01/13/2023-15:42:27] [I] Average on 10 runs - GPU latency: 2.14904 ms - Host latency: 3.31287 ms (enqueue 1.24341 ms)\n",
            "[01/13/2023-15:42:27] [I] Average on 10 runs - GPU latency: 2.12933 ms - Host latency: 3.29607 ms (enqueue 0.849475 ms)\n",
            "[01/13/2023-15:42:27] [I] Average on 10 runs - GPU latency: 2.23421 ms - Host latency: 3.41231 ms (enqueue 0.909882 ms)\n",
            "[01/13/2023-15:42:27] [I] Average on 10 runs - GPU latency: 2.24388 ms - Host latency: 3.44134 ms (enqueue 0.855695 ms)\n",
            "[01/13/2023-15:42:27] [I] Average on 10 runs - GPU latency: 2.22096 ms - Host latency: 3.38527 ms (enqueue 0.779157 ms)\n",
            "[01/13/2023-15:42:27] [I] Average on 10 runs - GPU latency: 2.19904 ms - Host latency: 3.3569 ms (enqueue 0.80719 ms)\n",
            "[01/13/2023-15:42:27] [I] Average on 10 runs - GPU latency: 2.17411 ms - Host latency: 3.32855 ms (enqueue 0.764777 ms)\n",
            "[01/13/2023-15:42:27] [I] Average on 10 runs - GPU latency: 2.15618 ms - Host latency: 3.31292 ms (enqueue 0.819128 ms)\n",
            "[01/13/2023-15:42:27] [I] Average on 10 runs - GPU latency: 2.14583 ms - Host latency: 3.31338 ms (enqueue 0.802924 ms)\n",
            "[01/13/2023-15:42:27] [I] Average on 10 runs - GPU latency: 2.14703 ms - Host latency: 3.3266 ms (enqueue 0.791956 ms)\n",
            "[01/13/2023-15:42:27] [I] Average on 10 runs - GPU latency: 2.14775 ms - Host latency: 3.31216 ms (enqueue 0.794116 ms)\n",
            "[01/13/2023-15:42:27] [I] Average on 10 runs - GPU latency: 2.14795 ms - Host latency: 3.30817 ms (enqueue 0.781348 ms)\n",
            "[01/13/2023-15:42:27] [I] Average on 10 runs - GPU latency: 2.13352 ms - Host latency: 3.29517 ms (enqueue 0.787158 ms)\n",
            "[01/13/2023-15:42:27] [I] Average on 10 runs - GPU latency: 2.12886 ms - Host latency: 3.28429 ms (enqueue 0.770312 ms)\n",
            "[01/13/2023-15:42:27] [I] Average on 10 runs - GPU latency: 2.16052 ms - Host latency: 3.31631 ms (enqueue 0.812061 ms)\n",
            "[01/13/2023-15:42:27] [I] Average on 10 runs - GPU latency: 2.16487 ms - Host latency: 3.32426 ms (enqueue 0.771484 ms)\n",
            "[01/13/2023-15:42:27] [I] Average on 10 runs - GPU latency: 2.16545 ms - Host latency: 3.32957 ms (enqueue 0.817505 ms)\n",
            "[01/13/2023-15:42:27] [I] Average on 10 runs - GPU latency: 2.1411 ms - Host latency: 3.30209 ms (enqueue 0.793909 ms)\n",
            "[01/13/2023-15:42:27] [I] Average on 10 runs - GPU latency: 2.14607 ms - Host latency: 3.29766 ms (enqueue 0.765259 ms)\n",
            "[01/13/2023-15:42:27] [I] Average on 10 runs - GPU latency: 2.14547 ms - Host latency: 3.30742 ms (enqueue 0.770569 ms)\n",
            "[01/13/2023-15:42:27] [I] Average on 10 runs - GPU latency: 2.16536 ms - Host latency: 3.33469 ms (enqueue 1.03534 ms)\n",
            "[01/13/2023-15:42:27] [I] Average on 10 runs - GPU latency: 2.16616 ms - Host latency: 3.33595 ms (enqueue 0.832947 ms)\n",
            "[01/13/2023-15:42:27] [I] Average on 10 runs - GPU latency: 2.14556 ms - Host latency: 3.30212 ms (enqueue 0.776855 ms)\n",
            "[01/13/2023-15:42:27] [I] Average on 10 runs - GPU latency: 2.14363 ms - Host latency: 3.29836 ms (enqueue 0.80625 ms)\n",
            "[01/13/2023-15:42:27] [I] Average on 10 runs - GPU latency: 2.14701 ms - Host latency: 3.3028 ms (enqueue 0.771875 ms)\n",
            "[01/13/2023-15:42:27] [I] Average on 10 runs - GPU latency: 2.14489 ms - Host latency: 3.30098 ms (enqueue 0.820435 ms)\n",
            "[01/13/2023-15:42:27] [I] Average on 10 runs - GPU latency: 2.13193 ms - Host latency: 3.29271 ms (enqueue 0.82854 ms)\n",
            "[01/13/2023-15:42:27] [I] Average on 10 runs - GPU latency: 2.12701 ms - Host latency: 3.32113 ms (enqueue 0.785217 ms)\n",
            "[01/13/2023-15:42:27] [I] Average on 10 runs - GPU latency: 2.12384 ms - Host latency: 3.28085 ms (enqueue 0.802905 ms)\n",
            "[01/13/2023-15:42:27] [I] Average on 10 runs - GPU latency: 2.15995 ms - Host latency: 3.31761 ms (enqueue 0.825623 ms)\n",
            "[01/13/2023-15:42:27] [I] Average on 10 runs - GPU latency: 2.16423 ms - Host latency: 3.32443 ms (enqueue 0.805847 ms)\n",
            "[01/13/2023-15:42:27] [I] Average on 10 runs - GPU latency: 2.16848 ms - Host latency: 3.3588 ms (enqueue 0.798291 ms)\n",
            "[01/13/2023-15:42:27] [I] Average on 10 runs - GPU latency: 2.16122 ms - Host latency: 3.32013 ms (enqueue 0.814111 ms)\n",
            "[01/13/2023-15:42:27] [I] Average on 10 runs - GPU latency: 2.14271 ms - Host latency: 3.29995 ms (enqueue 0.796216 ms)\n",
            "[01/13/2023-15:42:27] [I] Average on 10 runs - GPU latency: 2.15048 ms - Host latency: 3.31721 ms (enqueue 0.778784 ms)\n",
            "[01/13/2023-15:42:27] [I] Average on 10 runs - GPU latency: 2.14814 ms - Host latency: 3.32015 ms (enqueue 0.779651 ms)\n",
            "[01/13/2023-15:42:27] [I] Average on 10 runs - GPU latency: 2.14415 ms - Host latency: 3.30226 ms (enqueue 0.781103 ms)\n",
            "[01/13/2023-15:42:27] [I] Average on 10 runs - GPU latency: 2.13646 ms - Host latency: 3.29734 ms (enqueue 0.772583 ms)\n",
            "[01/13/2023-15:42:27] [I] Average on 10 runs - GPU latency: 2.11938 ms - Host latency: 3.28628 ms (enqueue 0.807874 ms)\n",
            "[01/13/2023-15:42:27] [I] Average on 10 runs - GPU latency: 2.12493 ms - Host latency: 3.30637 ms (enqueue 0.847656 ms)\n",
            "[01/13/2023-15:42:27] [I] Average on 10 runs - GPU latency: 2.14843 ms - Host latency: 3.31725 ms (enqueue 0.831055 ms)\n",
            "[01/13/2023-15:42:27] [I] Average on 10 runs - GPU latency: 2.16373 ms - Host latency: 3.32833 ms (enqueue 0.845874 ms)\n",
            "[01/13/2023-15:42:27] [I] Average on 10 runs - GPU latency: 2.17124 ms - Host latency: 3.38243 ms (enqueue 0.805078 ms)\n",
            "[01/13/2023-15:42:27] [I] Average on 10 runs - GPU latency: 2.17565 ms - Host latency: 3.38997 ms (enqueue 0.769312 ms)\n",
            "[01/13/2023-15:42:27] [I] Average on 10 runs - GPU latency: 2.16217 ms - Host latency: 3.33096 ms (enqueue 1.00516 ms)\n",
            "[01/13/2023-15:42:27] [I] Average on 10 runs - GPU latency: 2.14847 ms - Host latency: 3.31083 ms (enqueue 0.848682 ms)\n",
            "[01/13/2023-15:42:27] [I] Average on 10 runs - GPU latency: 2.18568 ms - Host latency: 3.36482 ms (enqueue 0.794409 ms)\n",
            "[01/13/2023-15:42:27] [I] Average on 10 runs - GPU latency: 2.17079 ms - Host latency: 3.34991 ms (enqueue 0.963037 ms)\n",
            "[01/13/2023-15:42:27] [I] Average on 10 runs - GPU latency: 2.16307 ms - Host latency: 3.31879 ms (enqueue 0.804114 ms)\n",
            "[01/13/2023-15:42:27] [I] Average on 10 runs - GPU latency: 2.14912 ms - Host latency: 3.30972 ms (enqueue 0.775562 ms)\n",
            "[01/13/2023-15:42:27] [I] Average on 10 runs - GPU latency: 2.14713 ms - Host latency: 3.30245 ms (enqueue 0.786255 ms)\n",
            "[01/13/2023-15:42:27] [I] Average on 10 runs - GPU latency: 2.1452 ms - Host latency: 3.31511 ms (enqueue 0.768042 ms)\n",
            "[01/13/2023-15:42:27] [I] Average on 10 runs - GPU latency: 2.14631 ms - Host latency: 3.3025 ms (enqueue 0.77572 ms)\n",
            "[01/13/2023-15:42:27] [I] Average on 10 runs - GPU latency: 2.13761 ms - Host latency: 3.29911 ms (enqueue 0.779858 ms)\n",
            "[01/13/2023-15:42:27] [I] Average on 10 runs - GPU latency: 2.15034 ms - Host latency: 3.30498 ms (enqueue 0.775208 ms)\n",
            "[01/13/2023-15:42:27] [I] Average on 10 runs - GPU latency: 2.16743 ms - Host latency: 3.32285 ms (enqueue 0.77262 ms)\n",
            "[01/13/2023-15:42:27] [I] Average on 10 runs - GPU latency: 2.14648 ms - Host latency: 3.31625 ms (enqueue 0.786523 ms)\n",
            "[01/13/2023-15:42:27] [I] Average on 10 runs - GPU latency: 2.14397 ms - Host latency: 3.30161 ms (enqueue 0.749536 ms)\n",
            "[01/13/2023-15:42:27] [I] Average on 10 runs - GPU latency: 2.16372 ms - Host latency: 3.32122 ms (enqueue 0.786597 ms)\n",
            "[01/13/2023-15:42:27] [I] Average on 10 runs - GPU latency: 2.16465 ms - Host latency: 3.31863 ms (enqueue 0.776001 ms)\n",
            "[01/13/2023-15:42:27] [I] Average on 10 runs - GPU latency: 2.16814 ms - Host latency: 3.3291 ms (enqueue 0.77063 ms)\n",
            "[01/13/2023-15:42:27] [I] Average on 10 runs - GPU latency: 2.15957 ms - Host latency: 3.36704 ms (enqueue 0.746631 ms)\n",
            "[01/13/2023-15:42:27] [I] Average on 10 runs - GPU latency: 2.14387 ms - Host latency: 3.30352 ms (enqueue 0.76582 ms)\n",
            "[01/13/2023-15:42:27] [I] Average on 10 runs - GPU latency: 2.14326 ms - Host latency: 3.30457 ms (enqueue 0.778394 ms)\n",
            "[01/13/2023-15:42:27] [I] Average on 10 runs - GPU latency: 2.14695 ms - Host latency: 3.30039 ms (enqueue 0.766992 ms)\n",
            "[01/13/2023-15:42:27] [I] Average on 10 runs - GPU latency: 2.14314 ms - Host latency: 3.3002 ms (enqueue 0.779443 ms)\n",
            "[01/13/2023-15:42:27] [I] Average on 10 runs - GPU latency: 2.13411 ms - Host latency: 3.28713 ms (enqueue 0.78457 ms)\n",
            "[01/13/2023-15:42:27] [I] Average on 10 runs - GPU latency: 2.12642 ms - Host latency: 3.28704 ms (enqueue 0.781397 ms)\n",
            "[01/13/2023-15:42:27] [I] Average on 10 runs - GPU latency: 2.12947 ms - Host latency: 3.28638 ms (enqueue 0.771313 ms)\n",
            "[01/13/2023-15:42:27] [I] Average on 10 runs - GPU latency: 2.12646 ms - Host latency: 3.29153 ms (enqueue 0.85376 ms)\n",
            "[01/13/2023-15:42:27] [I] Average on 10 runs - GPU latency: 2.13101 ms - Host latency: 3.28718 ms (enqueue 0.800635 ms)\n",
            "[01/13/2023-15:42:27] [I] Average on 10 runs - GPU latency: 2.15239 ms - Host latency: 3.36111 ms (enqueue 0.813013 ms)\n",
            "[01/13/2023-15:42:27] [I] Average on 10 runs - GPU latency: 2.14604 ms - Host latency: 3.30066 ms (enqueue 0.800806 ms)\n",
            "[01/13/2023-15:42:27] [I] Average on 10 runs - GPU latency: 2.14934 ms - Host latency: 3.35334 ms (enqueue 0.785107 ms)\n",
            "[01/13/2023-15:42:27] [I] Average on 10 runs - GPU latency: 2.15117 ms - Host latency: 3.31074 ms (enqueue 0.769751 ms)\n",
            "[01/13/2023-15:42:27] [I] Average on 10 runs - GPU latency: 2.14944 ms - Host latency: 3.30569 ms (enqueue 0.759033 ms)\n",
            "[01/13/2023-15:42:27] [I] Average on 10 runs - GPU latency: 2.13003 ms - Host latency: 3.28171 ms (enqueue 0.779395 ms)\n",
            "[01/13/2023-15:42:27] [I] Average on 10 runs - GPU latency: 2.16399 ms - Host latency: 3.31931 ms (enqueue 0.774023 ms)\n",
            "[01/13/2023-15:42:27] [I] Average on 10 runs - GPU latency: 2.16233 ms - Host latency: 3.31633 ms (enqueue 0.772119 ms)\n",
            "[01/13/2023-15:42:27] [I] Average on 10 runs - GPU latency: 2.16563 ms - Host latency: 3.32219 ms (enqueue 0.771973 ms)\n",
            "[01/13/2023-15:42:27] [I] Average on 10 runs - GPU latency: 2.14959 ms - Host latency: 3.30146 ms (enqueue 0.772583 ms)\n",
            "[01/13/2023-15:42:27] [I] Average on 10 runs - GPU latency: 2.1478 ms - Host latency: 3.30156 ms (enqueue 0.753564 ms)\n",
            "[01/13/2023-15:42:27] [I] Average on 10 runs - GPU latency: 2.14695 ms - Host latency: 3.30063 ms (enqueue 0.77395 ms)\n",
            "[01/13/2023-15:42:27] [I] Average on 10 runs - GPU latency: 2.14731 ms - Host latency: 3.30498 ms (enqueue 0.772119 ms)\n",
            "[01/13/2023-15:42:27] [I] Average on 10 runs - GPU latency: 2.1321 ms - Host latency: 3.28596 ms (enqueue 0.776587 ms)\n",
            "[01/13/2023-15:42:27] [I] Average on 10 runs - GPU latency: 2.13865 ms - Host latency: 3.29265 ms (enqueue 0.777759 ms)\n",
            "[01/13/2023-15:42:27] [I] Average on 10 runs - GPU latency: 2.16504 ms - Host latency: 3.32415 ms (enqueue 0.774292 ms)\n",
            "[01/13/2023-15:42:27] [I] Average on 10 runs - GPU latency: 2.16423 ms - Host latency: 3.34783 ms (enqueue 0.770581 ms)\n",
            "[01/13/2023-15:42:27] [I] Average on 10 runs - GPU latency: 2.16355 ms - Host latency: 3.32393 ms (enqueue 0.770776 ms)\n",
            "[01/13/2023-15:42:27] [I] Average on 10 runs - GPU latency: 2.16377 ms - Host latency: 3.31655 ms (enqueue 0.769385 ms)\n",
            "[01/13/2023-15:42:27] [I] Average on 10 runs - GPU latency: 2.15664 ms - Host latency: 3.30947 ms (enqueue 0.782031 ms)\n",
            "[01/13/2023-15:42:27] [I] Average on 10 runs - GPU latency: 2.13181 ms - Host latency: 3.30671 ms (enqueue 0.930835 ms)\n",
            "[01/13/2023-15:42:27] [I] Average on 10 runs - GPU latency: 2.15081 ms - Host latency: 3.30825 ms (enqueue 0.784448 ms)\n",
            "[01/13/2023-15:42:27] [I] Average on 10 runs - GPU latency: 2.17957 ms - Host latency: 3.34282 ms (enqueue 0.804736 ms)\n",
            "[01/13/2023-15:42:27] [I] Average on 10 runs - GPU latency: 2.17688 ms - Host latency: 3.35774 ms (enqueue 1.01494 ms)\n",
            "[01/13/2023-15:42:27] [I] Average on 10 runs - GPU latency: 2.17173 ms - Host latency: 3.35178 ms (enqueue 0.885278 ms)\n",
            "[01/13/2023-15:42:27] [I] Average on 10 runs - GPU latency: 2.15723 ms - Host latency: 3.32307 ms (enqueue 0.833765 ms)\n",
            "[01/13/2023-15:42:27] [I] Average on 10 runs - GPU latency: 2.14697 ms - Host latency: 3.34756 ms (enqueue 0.812451 ms)\n",
            "[01/13/2023-15:42:27] [I] Average on 10 runs - GPU latency: 2.14751 ms - Host latency: 3.30728 ms (enqueue 0.842212 ms)\n",
            "[01/13/2023-15:42:27] [I] Average on 10 runs - GPU latency: 2.14744 ms - Host latency: 3.34631 ms (enqueue 0.795654 ms)\n",
            "[01/13/2023-15:42:27] [I] Average on 10 runs - GPU latency: 2.14023 ms - Host latency: 3.30405 ms (enqueue 0.821045 ms)\n",
            "[01/13/2023-15:42:27] [I] Average on 10 runs - GPU latency: 2.12695 ms - Host latency: 3.29458 ms (enqueue 0.803052 ms)\n",
            "[01/13/2023-15:42:27] [I] Average on 10 runs - GPU latency: 2.16748 ms - Host latency: 3.33127 ms (enqueue 0.793701 ms)\n",
            "[01/13/2023-15:42:27] [I] Average on 10 runs - GPU latency: 2.16563 ms - Host latency: 3.32893 ms (enqueue 0.78877 ms)\n",
            "[01/13/2023-15:42:27] [I] Average on 10 runs - GPU latency: 2.16721 ms - Host latency: 3.32898 ms (enqueue 0.822388 ms)\n",
            "[01/13/2023-15:42:27] [I] Average on 10 runs - GPU latency: 2.16143 ms - Host latency: 3.31348 ms (enqueue 0.804224 ms)\n",
            "[01/13/2023-15:42:27] [I] Average on 10 runs - GPU latency: 2.16079 ms - Host latency: 3.31748 ms (enqueue 0.805273 ms)\n",
            "[01/13/2023-15:42:27] [I] Average on 10 runs - GPU latency: 2.15437 ms - Host latency: 3.30764 ms (enqueue 0.790649 ms)\n",
            "[01/13/2023-15:42:27] [I] Average on 10 runs - GPU latency: 2.16365 ms - Host latency: 3.32668 ms (enqueue 0.82102 ms)\n",
            "[01/13/2023-15:42:27] [I] \n",
            "[01/13/2023-15:42:27] [I] === Performance summary ===\n",
            "[01/13/2023-15:42:27] [I] Throughput: 461.947 qps\n",
            "[01/13/2023-15:42:27] [I] Latency: min = 3.24017 ms, max = 3.79962 ms, mean = 3.3172 ms, median = 3.30811 ms, percentile(90%) = 3.354 ms, percentile(95%) = 3.38599 ms, percentile(99%) = 3.58768 ms\n",
            "[01/13/2023-15:42:27] [I] Enqueue Time: min = 0.689941 ms, max = 3.09772 ms, mean = 0.81553 ms, median = 0.789307 ms, percentile(90%) = 0.866699 ms, percentile(95%) = 1.00012 ms, percentile(99%) = 1.34668 ms\n",
            "[01/13/2023-15:42:27] [I] H2D Latency: min = 0.429413 ms, max = 0.840057 ms, mean = 0.446008 ms, median = 0.436523 ms, percentile(90%) = 0.451172 ms, percentile(95%) = 0.490234 ms, percentile(99%) = 0.709259 ms\n",
            "[01/13/2023-15:42:27] [I] GPU Compute Time: min = 2.08487 ms, max = 2.28992 ms, mean = 2.15063 ms, median = 2.1488 ms, percentile(90%) = 2.17285 ms, percentile(95%) = 2.1853 ms, percentile(99%) = 2.2345 ms\n",
            "[01/13/2023-15:42:27] [I] D2H Latency: min = 0.703125 ms, max = 0.811035 ms, mean = 0.720568 ms, median = 0.718994 ms, percentile(90%) = 0.724792 ms, percentile(95%) = 0.72818 ms, percentile(99%) = 0.783875 ms\n",
            "[01/13/2023-15:42:27] [I] Total Host Walltime: 3.00684 s\n",
            "[01/13/2023-15:42:27] [I] Total GPU Compute Time: 2.98722 s\n",
            "[01/13/2023-15:42:27] [W] * GPU compute time is unstable, with coefficient of variance = 1.05379%.\n",
            "[01/13/2023-15:42:27] [W]   If not already in use, locking GPU clock frequency or adding --useSpinWait may improve the stability.\n",
            "[01/13/2023-15:42:27] [I] Explanations of the performance metrics are printed in the verbose logs.\n",
            "[01/13/2023-15:42:27] [I] \n",
            "&&&& PASSED TensorRT.trtexec [TensorRT v8502] # /usr/src/tensorrt/bin/trtexec --onnx=/content/yolov7/yolov7-tiny.onnx --saveEngine=/content/yolov7.trt --fp16\n"
          ]
        }
      ],
      "source": [
        "# Create engine\n",
        "!/usr/src/tensorrt/bin/trtexec --onnx=/content/yolov7/yolov7-tiny.onnx --saveEngine=/content/yolov7.trt --fp16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aq_q1JsDrl3G",
        "outputId": "f4d9bc56-77d3-46e0-e2cd-2d49436c1d36"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "&&&& RUNNING TensorRT.trtexec [TensorRT v8502] # /usr/src/tensorrt/bin/trtexec --loadEngine=/content/yolov7.trt --batch=1 --warmUp=500 --duration=1 --iterations=500 --streams=1 --useSpinWait\n",
            "[01/13/2023-15:42:27] [I] === Model Options ===\n",
            "[01/13/2023-15:42:27] [I] Format: *\n",
            "[01/13/2023-15:42:27] [I] Model: \n",
            "[01/13/2023-15:42:27] [I] Output:\n",
            "[01/13/2023-15:42:27] [I] === Build Options ===\n",
            "[01/13/2023-15:42:27] [I] Max batch: 1\n",
            "[01/13/2023-15:42:27] [I] Memory Pools: workspace: default, dlaSRAM: default, dlaLocalDRAM: default, dlaGlobalDRAM: default\n",
            "[01/13/2023-15:42:27] [I] minTiming: 1\n",
            "[01/13/2023-15:42:27] [I] avgTiming: 8\n",
            "[01/13/2023-15:42:27] [I] Precision: FP32\n",
            "[01/13/2023-15:42:27] [I] LayerPrecisions: \n",
            "[01/13/2023-15:42:27] [I] Calibration: \n",
            "[01/13/2023-15:42:27] [I] Refit: Disabled\n",
            "[01/13/2023-15:42:27] [I] Sparsity: Disabled\n",
            "[01/13/2023-15:42:27] [I] Safe mode: Disabled\n",
            "[01/13/2023-15:42:27] [I] DirectIO mode: Disabled\n",
            "[01/13/2023-15:42:27] [I] Restricted mode: Disabled\n",
            "[01/13/2023-15:42:27] [I] Build only: Disabled\n",
            "[01/13/2023-15:42:27] [I] Save engine: \n",
            "[01/13/2023-15:42:27] [I] Load engine: /content/yolov7.trt\n",
            "[01/13/2023-15:42:27] [I] Profiling verbosity: 0\n",
            "[01/13/2023-15:42:27] [I] Tactic sources: Using default tactic sources\n",
            "[01/13/2023-15:42:27] [I] timingCacheMode: local\n",
            "[01/13/2023-15:42:27] [I] timingCacheFile: \n",
            "[01/13/2023-15:42:27] [I] Heuristic: Disabled\n",
            "[01/13/2023-15:42:27] [I] Preview Features: Use default preview flags.\n",
            "[01/13/2023-15:42:27] [I] Input(s)s format: fp32:CHW\n",
            "[01/13/2023-15:42:27] [I] Output(s)s format: fp32:CHW\n",
            "[01/13/2023-15:42:27] [I] Input build shapes: model\n",
            "[01/13/2023-15:42:27] [I] Input calibration shapes: model\n",
            "[01/13/2023-15:42:27] [I] === System Options ===\n",
            "[01/13/2023-15:42:27] [I] Device: 0\n",
            "[01/13/2023-15:42:27] [I] DLACore: \n",
            "[01/13/2023-15:42:27] [I] Plugins:\n",
            "[01/13/2023-15:42:27] [I] === Inference Options ===\n",
            "[01/13/2023-15:42:27] [I] Batch: 1\n",
            "[01/13/2023-15:42:27] [I] Input inference shapes: model\n",
            "[01/13/2023-15:42:27] [I] Iterations: 500\n",
            "[01/13/2023-15:42:27] [I] Duration: 1s (+ 500ms warm up)\n",
            "[01/13/2023-15:42:27] [I] Sleep time: 0ms\n",
            "[01/13/2023-15:42:27] [I] Idle time: 0ms\n",
            "[01/13/2023-15:42:27] [I] Streams: 1\n",
            "[01/13/2023-15:42:27] [I] ExposeDMA: Disabled\n",
            "[01/13/2023-15:42:27] [I] Data transfers: Enabled\n",
            "[01/13/2023-15:42:27] [I] Spin-wait: Enabled\n",
            "[01/13/2023-15:42:27] [I] Multithreading: Disabled\n",
            "[01/13/2023-15:42:27] [I] CUDA Graph: Disabled\n",
            "[01/13/2023-15:42:27] [I] Separate profiling: Disabled\n",
            "[01/13/2023-15:42:27] [I] Time Deserialize: Disabled\n",
            "[01/13/2023-15:42:27] [I] Time Refit: Disabled\n",
            "[01/13/2023-15:42:27] [I] NVTX verbosity: 0\n",
            "[01/13/2023-15:42:27] [I] Persistent Cache Ratio: 0\n",
            "[01/13/2023-15:42:27] [I] Inputs:\n",
            "[01/13/2023-15:42:27] [I] === Reporting Options ===\n",
            "[01/13/2023-15:42:27] [I] Verbose: Disabled\n",
            "[01/13/2023-15:42:27] [I] Averages: 10 inferences\n",
            "[01/13/2023-15:42:27] [I] Percentiles: 90,95,99\n",
            "[01/13/2023-15:42:27] [I] Dump refittable layers:Disabled\n",
            "[01/13/2023-15:42:27] [I] Dump output: Disabled\n",
            "[01/13/2023-15:42:27] [I] Profile: Disabled\n",
            "[01/13/2023-15:42:27] [I] Export timing to JSON file: \n",
            "[01/13/2023-15:42:27] [I] Export output to JSON file: \n",
            "[01/13/2023-15:42:27] [I] Export profile to JSON file: \n",
            "[01/13/2023-15:42:27] [I] \n",
            "[01/13/2023-15:42:27] [I] === Device Information ===\n",
            "[01/13/2023-15:42:27] [I] Selected Device: Tesla T4\n",
            "[01/13/2023-15:42:27] [I] Compute Capability: 7.5\n",
            "[01/13/2023-15:42:27] [I] SMs: 40\n",
            "[01/13/2023-15:42:27] [I] Compute Clock Rate: 1.59 GHz\n",
            "[01/13/2023-15:42:27] [I] Device Global Memory: 15109 MiB\n",
            "[01/13/2023-15:42:27] [I] Shared Memory per SM: 64 KiB\n",
            "[01/13/2023-15:42:27] [I] Memory Bus Width: 256 bits (ECC enabled)\n",
            "[01/13/2023-15:42:27] [I] Memory Clock Rate: 5.001 GHz\n",
            "[01/13/2023-15:42:27] [I] \n",
            "[01/13/2023-15:42:27] [I] TensorRT version: 8.5.2\n",
            "[01/13/2023-15:42:27] [I] Engine loaded in 0.0209839 sec.\n",
            "[01/13/2023-15:42:28] [I] [TRT] Loaded engine size: 13 MiB\n",
            "[01/13/2023-15:42:29] [I] [TRT] [MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +0, GPU +12, now: CPU 0, GPU 12 (MiB)\n",
            "[01/13/2023-15:42:29] [I] Engine deserialized in 1.51958 sec.\n",
            "[01/13/2023-15:42:29] [I] [TRT] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +19, now: CPU 0, GPU 31 (MiB)\n",
            "[01/13/2023-15:42:29] [W] [TRT] CUDA lazy loading is not enabled. Enabling it can significantly reduce device memory usage. See `CUDA_MODULE_LOADING` in https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#env-vars\n",
            "[01/13/2023-15:42:29] [I] Setting persistentCacheLimit to 0 bytes.\n",
            "[01/13/2023-15:42:29] [I] Using random values for input images\n",
            "[01/13/2023-15:42:29] [I] Created input binding for images with dimensions 1x3x640x640\n",
            "[01/13/2023-15:42:29] [I] Using random values for output output\n",
            "[01/13/2023-15:42:29] [I] Created output binding for output with dimensions 1x25200x85\n",
            "[01/13/2023-15:42:29] [I] Starting inference\n",
            "[01/13/2023-15:42:31] [I] Warmup completed 176 queries over 500 ms\n",
            "[01/13/2023-15:42:31] [I] Timing trace has 500 queries over 1.10009 s\n",
            "[01/13/2023-15:42:31] [I] \n",
            "[01/13/2023-15:42:31] [I] === Trace details ===\n",
            "[01/13/2023-15:42:31] [I] Trace averages of 10 runs:\n",
            "[01/13/2023-15:42:31] [I] Average on 10 runs - GPU latency: 2.17117 ms - Host latency: 3.52715 ms (enqueue 0.637054 ms)\n",
            "[01/13/2023-15:42:31] [I] Average on 10 runs - GPU latency: 2.19403 ms - Host latency: 3.5521 ms (enqueue 0.647876 ms)\n",
            "[01/13/2023-15:42:31] [I] Average on 10 runs - GPU latency: 2.19531 ms - Host latency: 3.55074 ms (enqueue 0.645459 ms)\n",
            "[01/13/2023-15:42:31] [I] Average on 10 runs - GPU latency: 2.19412 ms - Host latency: 3.54592 ms (enqueue 0.64577 ms)\n",
            "[01/13/2023-15:42:31] [I] Average on 10 runs - GPU latency: 2.18018 ms - Host latency: 3.54175 ms (enqueue 0.7039 ms)\n",
            "[01/13/2023-15:42:31] [I] Average on 10 runs - GPU latency: 2.17178 ms - Host latency: 3.52708 ms (enqueue 0.637128 ms)\n",
            "[01/13/2023-15:42:31] [I] Average on 10 runs - GPU latency: 2.17897 ms - Host latency: 3.53309 ms (enqueue 0.651459 ms)\n",
            "[01/13/2023-15:42:31] [I] Average on 10 runs - GPU latency: 2.17612 ms - Host latency: 3.52735 ms (enqueue 0.628931 ms)\n",
            "[01/13/2023-15:42:31] [I] Average on 10 runs - GPU latency: 2.16822 ms - Host latency: 3.52289 ms (enqueue 0.650189 ms)\n",
            "[01/13/2023-15:42:31] [I] Average on 10 runs - GPU latency: 2.15275 ms - Host latency: 3.51315 ms (enqueue 0.649432 ms)\n",
            "[01/13/2023-15:42:31] [I] Average on 10 runs - GPU latency: 2.15782 ms - Host latency: 3.51889 ms (enqueue 0.644495 ms)\n",
            "[01/13/2023-15:42:31] [I] Average on 10 runs - GPU latency: 2.16967 ms - Host latency: 3.54954 ms (enqueue 0.743414 ms)\n",
            "[01/13/2023-15:42:31] [I] Average on 10 runs - GPU latency: 2.19219 ms - Host latency: 3.5481 ms (enqueue 0.654327 ms)\n",
            "[01/13/2023-15:42:31] [I] Average on 10 runs - GPU latency: 2.20263 ms - Host latency: 3.57729 ms (enqueue 0.686951 ms)\n",
            "[01/13/2023-15:42:31] [I] Average on 10 runs - GPU latency: 2.19434 ms - Host latency: 3.58278 ms (enqueue 0.637738 ms)\n",
            "[01/13/2023-15:42:31] [I] Average on 10 runs - GPU latency: 2.18199 ms - Host latency: 3.55533 ms (enqueue 0.655286 ms)\n",
            "[01/13/2023-15:42:31] [I] Average on 10 runs - GPU latency: 2.17599 ms - Host latency: 3.5299 ms (enqueue 0.643121 ms)\n",
            "[01/13/2023-15:42:31] [I] Average on 10 runs - GPU latency: 2.17975 ms - Host latency: 3.53658 ms (enqueue 0.624445 ms)\n",
            "[01/13/2023-15:42:31] [I] Average on 10 runs - GPU latency: 2.17346 ms - Host latency: 3.52598 ms (enqueue 0.624548 ms)\n",
            "[01/13/2023-15:42:31] [I] Average on 10 runs - GPU latency: 2.17443 ms - Host latency: 3.53303 ms (enqueue 0.65423 ms)\n",
            "[01/13/2023-15:42:31] [I] Average on 10 runs - GPU latency: 2.16376 ms - Host latency: 3.54493 ms (enqueue 0.667084 ms)\n",
            "[01/13/2023-15:42:31] [I] Average on 10 runs - GPU latency: 2.15875 ms - Host latency: 3.52286 ms (enqueue 0.635291 ms)\n",
            "[01/13/2023-15:42:31] [I] Average on 10 runs - GPU latency: 2.19017 ms - Host latency: 3.55042 ms (enqueue 0.868744 ms)\n",
            "[01/13/2023-15:42:31] [I] Average on 10 runs - GPU latency: 2.19224 ms - Host latency: 3.5561 ms (enqueue 0.709216 ms)\n",
            "[01/13/2023-15:42:31] [I] Average on 10 runs - GPU latency: 2.19033 ms - Host latency: 3.54348 ms (enqueue 0.65907 ms)\n",
            "[01/13/2023-15:42:31] [I] Average on 10 runs - GPU latency: 2.18339 ms - Host latency: 3.53461 ms (enqueue 0.658875 ms)\n",
            "[01/13/2023-15:42:31] [I] Average on 10 runs - GPU latency: 2.17476 ms - Host latency: 3.53196 ms (enqueue 0.636401 ms)\n",
            "[01/13/2023-15:42:31] [I] Average on 10 runs - GPU latency: 2.17644 ms - Host latency: 3.53003 ms (enqueue 0.656482 ms)\n",
            "[01/13/2023-15:42:31] [I] Average on 10 runs - GPU latency: 2.17501 ms - Host latency: 3.52648 ms (enqueue 0.641919 ms)\n",
            "[01/13/2023-15:42:31] [I] Average on 10 runs - GPU latency: 2.19075 ms - Host latency: 3.5443 ms (enqueue 0.700195 ms)\n",
            "[01/13/2023-15:42:31] [I] Average on 10 runs - GPU latency: 2.23824 ms - Host latency: 3.59614 ms (enqueue 0.65719 ms)\n",
            "[01/13/2023-15:42:31] [I] Average on 10 runs - GPU latency: 2.29315 ms - Host latency: 3.67744 ms (enqueue 0.658569 ms)\n",
            "[01/13/2023-15:42:31] [I] Average on 10 runs - GPU latency: 2.2916 ms - Host latency: 3.65072 ms (enqueue 0.66477 ms)\n",
            "[01/13/2023-15:42:31] [I] Average on 10 runs - GPU latency: 2.25371 ms - Host latency: 3.60839 ms (enqueue 0.670508 ms)\n",
            "[01/13/2023-15:42:31] [I] Average on 10 runs - GPU latency: 2.25121 ms - Host latency: 3.63364 ms (enqueue 0.774036 ms)\n",
            "[01/13/2023-15:42:31] [I] Average on 10 runs - GPU latency: 2.23802 ms - Host latency: 3.59056 ms (enqueue 0.65387 ms)\n",
            "[01/13/2023-15:42:31] [I] Average on 10 runs - GPU latency: 2.21658 ms - Host latency: 3.56875 ms (enqueue 0.694409 ms)\n",
            "[01/13/2023-15:42:31] [I] Average on 10 runs - GPU latency: 2.19413 ms - Host latency: 3.55048 ms (enqueue 0.671374 ms)\n",
            "[01/13/2023-15:42:31] [I] Average on 10 runs - GPU latency: 2.19257 ms - Host latency: 3.55221 ms (enqueue 0.718176 ms)\n",
            "[01/13/2023-15:42:31] [I] Average on 10 runs - GPU latency: 2.19392 ms - Host latency: 3.58024 ms (enqueue 0.95575 ms)\n",
            "[01/13/2023-15:42:31] [I] Average on 10 runs - GPU latency: 2.19164 ms - Host latency: 3.54656 ms (enqueue 0.658203 ms)\n",
            "[01/13/2023-15:42:31] [I] Average on 10 runs - GPU latency: 2.18812 ms - Host latency: 3.53904 ms (enqueue 0.620593 ms)\n",
            "[01/13/2023-15:42:31] [I] Average on 10 runs - GPU latency: 2.17205 ms - Host latency: 3.52787 ms (enqueue 0.648169 ms)\n",
            "[01/13/2023-15:42:31] [I] Average on 10 runs - GPU latency: 2.1788 ms - Host latency: 3.53557 ms (enqueue 0.678711 ms)\n",
            "[01/13/2023-15:42:31] [I] Average on 10 runs - GPU latency: 2.21263 ms - Host latency: 3.57207 ms (enqueue 0.656567 ms)\n",
            "[01/13/2023-15:42:31] [I] Average on 10 runs - GPU latency: 2.21427 ms - Host latency: 3.60455 ms (enqueue 0.722424 ms)\n",
            "[01/13/2023-15:42:31] [I] Average on 10 runs - GPU latency: 2.21035 ms - Host latency: 3.57102 ms (enqueue 0.81228 ms)\n",
            "[01/13/2023-15:42:31] [I] Average on 10 runs - GPU latency: 2.19655 ms - Host latency: 3.5514 ms (enqueue 0.649353 ms)\n",
            "[01/13/2023-15:42:31] [I] Average on 10 runs - GPU latency: 2.18875 ms - Host latency: 3.53551 ms (enqueue 0.606628 ms)\n",
            "[01/13/2023-15:42:31] [I] Average on 10 runs - GPU latency: 2.18612 ms - Host latency: 3.5343 ms (enqueue 0.659692 ms)\n",
            "[01/13/2023-15:42:31] [I] \n",
            "[01/13/2023-15:42:31] [I] === Performance summary ===\n",
            "[01/13/2023-15:42:31] [I] Throughput: 454.508 qps\n",
            "[01/13/2023-15:42:31] [I] Latency: min = 3.47937 ms, max = 3.94958 ms, mean = 3.55421 ms, median = 3.54456 ms, percentile(90%) = 3.60291 ms, percentile(95%) = 3.63037 ms, percentile(99%) = 3.82068 ms\n",
            "[01/13/2023-15:42:31] [I] Enqueue Time: min = 0.58667 ms, max = 1.11743 ms, mean = 0.674606 ms, median = 0.647156 ms, percentile(90%) = 0.752197 ms, percentile(95%) = 0.932617 ms, percentile(99%) = 1.04425 ms\n",
            "[01/13/2023-15:42:31] [I] H2D Latency: min = 0.516479 ms, max = 0.828918 ms, mean = 0.536091 ms, median = 0.530426 ms, percentile(90%) = 0.542236 ms, percentile(95%) = 0.549316 ms, percentile(99%) = 0.822632 ms\n",
            "[01/13/2023-15:42:31] [I] GPU Compute Time: min = 2.13992 ms, max = 2.32239 ms, mean = 2.19366 ms, median = 2.18768 ms, percentile(90%) = 2.24048 ms, percentile(95%) = 2.25671 ms, percentile(99%) = 2.30127 ms\n",
            "[01/13/2023-15:42:31] [I] D2H Latency: min = 0.811157 ms, max = 0.854126 ms, mean = 0.824456 ms, median = 0.824097 ms, percentile(90%) = 0.830688 ms, percentile(95%) = 0.833496 ms, percentile(99%) = 0.841064 ms\n",
            "[01/13/2023-15:42:31] [I] Total Host Walltime: 1.10009 s\n",
            "[01/13/2023-15:42:31] [I] Total GPU Compute Time: 1.09683 s\n",
            "[01/13/2023-15:42:31] [W] * GPU compute time is unstable, with coefficient of variance = 1.46075%.\n",
            "[01/13/2023-15:42:31] [W]   If not already in use, locking GPU clock frequency or adding --useSpinWait may improve the stability.\n",
            "[01/13/2023-15:42:31] [I] Explanations of the performance metrics are printed in the verbose logs.\n",
            "[01/13/2023-15:42:31] [I] \n",
            "&&&& PASSED TensorRT.trtexec [TensorRT v8502] # /usr/src/tensorrt/bin/trtexec --loadEngine=/content/yolov7.trt --batch=1 --warmUp=500 --duration=1 --iterations=500 --streams=1 --useSpinWait\n"
          ]
        }
      ],
      "source": [
        "# Run speed test\n",
        "!/usr/src/tensorrt/bin/trtexec --loadEngine=/content/yolov7.trt --batch=1 --warmUp=500 --duration=1 --iterations=500 --streams=1 --useSpinWait"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Results\n",
        "Math bigger host to device time in YOLO without NMS doesn't compensate additional GPU time in version with NMS. Transfer all output tensor is very expensive in compare with only bounding boxes.  \n",
        "\n",
        "**With NMS:**\n",
        "- Latency: mean = **2.6587** ms\n",
        "- GPU Compute Time: mean = 2.19824 ms\n",
        "- D2H Latency: min = mean = 0.010955 ms\n",
        "\n",
        "**Withiot NMS:**\n",
        "- Latency: mean = **3.55421** ms\n",
        "- GPU Compute Time: mean = 2.19366 ms\n",
        "- D2H Latency: min = mean = 0.824456 ms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "â”â”â”â”â”â”â”â”â”â”â”â”â”â”¯â”â”â”â”â”â”â”â”¯â”â”â”â”â”â”â”â”¯â”â”â”â”â”â”â”â”â”â”â”â”‘\n",
            "â”‚ msec -->   â”‚   GPU â”‚   D2H â”‚   Latency â”‚\n",
            "â”â”â”â”â”â”â”â”â”â”â”â”â”â”¿â”â”â”â”â”â”â”â”¿â”â”â”â”â”â”â”â”¿â”â”â”â”â”â”â”â”â”â”â”â”¥\n",
            "â”‚ with NMS   â”‚ 2.194 â”‚ 0.011 â”‚     2.659 â”‚\n",
            "â”‚ wout NMS   â”‚ 2.198 â”‚ 0.824 â”‚     3.554 â”‚\n",
            "â”•â”â”â”â”â”â”â”â”â”â”â”â”â”·â”â”â”â”â”â”â”â”·â”â”â”â”â”â”â”â”·â”â”â”â”â”â”â”â”â”â”â”â”™\n"
          ]
        }
      ],
      "source": [
        "from tabulate import tabulate\n",
        "\n",
        "print(tabulate([[\"with NMS\", 2.194, 0.011, 2.659], [\"wout NMS\", 2.198, 0.824, 3.554]],[\"msec -->\", \"GPU\", \"D2H\", \"Latency\"], \"mixed_outline\"))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.6 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
