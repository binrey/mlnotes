{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sSDOngglBk_O",
        "outputId": "9c63c795-5e74-445d-969b-c095d28de542"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade setuptools pip --user\n",
        "!pip install --ignore-installed PyYAML\n",
        "!pip install Pillow\n",
        "\n",
        "!pip install nvidia-pyindex\n",
        "!pip install --upgrade nvidia-tensorrt\n",
        "!pip install pycuda\n",
        "\n",
        "!pip install protobuf<4.21.3\n",
        "!pip install onnxruntime-gpu\n",
        "!pip install onnx>=1.9.0\n",
        "!pip install onnx-simplifier>=0.3.6 --user"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hQ5fNost-gZI",
        "outputId": "7be7a416-abd9-40f0-8166-6f1679edcd65"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Python version: 3.8.16 (default, Dec  7 2022, 01:12:13) \n",
            "[GCC 7.5.0], sys.version_info(major=3, minor=8, micro=16, releaselevel='final', serial=0) \n",
            "Pytorch version: 1.13.0+cu116 \n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import torch\n",
        "print(f\"Python version: {sys.version}, {sys.version_info} \")\n",
        "print(f\"Pytorch version: {torch.__version__} \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "feCaRUEI-_Os",
        "outputId": "131ec518-3de8-4906-fa8e-93f9e10ddf89"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fri Jan 13 13:47:58 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   38C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yfZALjuo-_Md",
        "outputId": "d4fab30d-77ba-4b94-c5b4-10c60a28871f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'yolov7'...\n",
            "remote: Enumerating objects: 1127, done.\u001b[K\n",
            "remote: Total 1127 (delta 0), reused 0 (delta 0), pack-reused 1127\u001b[K\n",
            "Receiving objects: 100% (1127/1127), 69.93 MiB | 16.27 MiB/s, done.\n",
            "Resolving deltas: 100% (525/525), done.\n",
            "/content/yolov7\n",
            "cfg\tdetect.py  hubconf.py  models\t  requirements.txt  tools\t  utils\n",
            "data\texport.py  inference   paper\t  scripts\t    train_aux.py\n",
            "deploy\tfigure\t   LICENSE.md  README.md  test.py\t    train.py\n"
          ]
        }
      ],
      "source": [
        "!# Download YOLOv7 code\n",
        "!git clone https://github.com/WongKinYiu/yolov7\n",
        "%cd yolov7\n",
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eWlHa1NJ-_Jw",
        "outputId": "3e8b98b7-c1d2-4ecd-80f1-6e0fc5071131"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2023-01-13 13:51:39--  https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7-tiny.pt\n",
            "Resolving github.com (github.com)... 20.205.243.166\n",
            "Connecting to github.com (github.com)|20.205.243.166|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/511187726/ba7d01ee-125a-4134-8864-fa1abcbf94d5?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20230113%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20230113T135139Z&X-Amz-Expires=300&X-Amz-Signature=a1f04ef4c809d81128baf1f58b571cfdf0fda6ce6aa4c89f198a2e3a28e494f8&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=511187726&response-content-disposition=attachment%3B%20filename%3Dyolov7-tiny.pt&response-content-type=application%2Foctet-stream [following]\n",
            "--2023-01-13 13:51:39--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/511187726/ba7d01ee-125a-4134-8864-fa1abcbf94d5?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20230113%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20230113T135139Z&X-Amz-Expires=300&X-Amz-Signature=a1f04ef4c809d81128baf1f58b571cfdf0fda6ce6aa4c89f198a2e3a28e494f8&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=511187726&response-content-disposition=attachment%3B%20filename%3Dyolov7-tiny.pt&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 12639769 (12M) [application/octet-stream]\n",
            "Saving to: â€˜yolov7-tiny.ptâ€™\n",
            "\n",
            "yolov7-tiny.pt      100%[===================>]  12.05M  7.52MB/s    in 1.6s    \n",
            "\n",
            "2023-01-13 13:51:42 (7.52 MB/s) - â€˜yolov7-tiny.ptâ€™ saved [12639769/12639769]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!# Download trained weights\n",
        "!wget https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7-tiny.pt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "daqrFRRjgBUL"
      },
      "outputs": [],
      "source": [
        "%cd /usr/src/tensorrt/samples/trtexec\n",
        "!make"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test YOLOv7 with NMS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YyRB8favmXys",
        "outputId": "9e073b92-158c-46f1-eec2-4b3b44b59745"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/yolov7\n",
            "Import onnx_graphsurgeon failure: No module named 'onnx_graphsurgeon'\n",
            "Namespace(batch_size=1, conf_thres=0.35, device='cpu', dynamic=False, dynamic_batch=False, end2end=True, fp16=False, grid=True, img_size=[640, 640], include_nms=False, int8=False, iou_thres=0.65, max_wh=None, simplify=True, topk_all=100, weights='./yolov7-tiny.pt')\n",
            "YOLOR ðŸš€ v0.1-121-g2fdc7f1 torch 1.13.0+cu116 CPU\n",
            "\n",
            "Fusing layers... \n",
            "Model Summary: 200 layers, 6219709 parameters, 6219709 gradients\n",
            "/usr/local/lib/python3.8/dist-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3190.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
            "\n",
            "Starting TorchScript export with torch 1.13.0+cu116...\n",
            "/content/yolov7/models/yolo.py:52: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  if self.grid[i].shape[2:4] != x[i].shape[2:4]:\n",
            "TorchScript export success, saved as ./yolov7-tiny.torchscript.pt\n",
            "CoreML export failure: No module named 'coremltools'\n",
            "\n",
            "Starting TorchScript-Lite export with torch 1.13.0+cu116...\n",
            "TorchScript-Lite export success, saved as ./yolov7-tiny.torchscript.ptl\n",
            "\n",
            "Starting ONNX export with onnx 1.13.0...\n",
            "\n",
            "Starting export end2end onnx model for TensorRT...\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:673: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at aten/src/ATen/core/TensorBody.h:480.)\n",
            "  if param.grad is not None:\n",
            "/content/yolov7/models/experimental.py:144: FutureWarning: 'torch.onnx._patch_torch._graph_op' is deprecated in version 1.13 and will be removed in version 1.14. Please note 'g.op()' is to be removed from torch.Graph. Please open a GitHub issue if you need this functionality..\n",
            "  out = g.op(\"TRT::EfficientNMS_TRT\",\n",
            "/usr/local/lib/python3.8/dist-packages/torch/onnx/_patch_torch.py:81: UserWarning: The shape inference of TRT::EfficientNMS_TRT type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (Triggered internally at ../torch/csrc/jit/passes/onnx/shape_type_inference.cpp:1884.)\n",
            "  _C._jit_pass_onnx_node_shape_type_inference(\n",
            "/usr/local/lib/python3.8/dist-packages/torch/onnx/utils.py:687: UserWarning: The shape inference of TRT::EfficientNMS_TRT type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (Triggered internally at ../torch/csrc/jit/passes/onnx/shape_type_inference.cpp:1884.)\n",
            "  _C._jit_pass_onnx_graph_shape_type_inference(\n",
            "/usr/local/lib/python3.8/dist-packages/torch/onnx/utils.py:1178: UserWarning: The shape inference of TRT::EfficientNMS_TRT type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (Triggered internally at ../torch/csrc/jit/passes/onnx/shape_type_inference.cpp:1884.)\n",
            "  _C._jit_pass_onnx_graph_shape_type_inference(\n",
            "\n",
            "Starting to simplify ONNX...\n",
            "ONNX export success, saved as ./yolov7-tiny.onnx\n",
            "\n",
            "Export complete (7.57s). Visualize with https://github.com/lutzroeder/netron.\n"
          ]
        }
      ],
      "source": [
        "# Save temp ONN model with NMS layer\n",
        "%cd /content/yolov7/\n",
        "!python export.py --weights ./yolov7-tiny.pt --grid --end2end --simplify --topk-all 100 --iou-thres 0.65 --conf-thres 0.35 --img-size 640 640"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t4clwTYTcbfl",
        "outputId": "6d91d4f1-162b-4960-8d13-74cd40bacf67"
      },
      "outputs": [],
      "source": [
        "# Create engine\n",
        "!/usr/src/tensorrt/bin/trtexec --onnx=/content/yolov7/yolov7-tiny.onnx --saveEngine=/content/yolov7nms.trt --fp16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HnhF0aH1vUJC",
        "outputId": "0078dc79-c33e-410c-b152-1f88d64348f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "&&&& RUNNING TensorRT.trtexec [TensorRT v8502] # /usr/src/tensorrt/bin/trtexec --loadEngine=/content/yolov7nms.trt --batch=1 --warmUp=500 --duration=1 --iterations=500 --streams=1 --useSpinWait\n",
            "[01/13/2023-15:26:36] [I] === Model Options ===\n",
            "[01/13/2023-15:26:36] [I] Format: *\n",
            "[01/13/2023-15:26:36] [I] Model: \n",
            "[01/13/2023-15:26:36] [I] Output:\n",
            "[01/13/2023-15:26:36] [I] === Build Options ===\n",
            "[01/13/2023-15:26:36] [I] Max batch: 1\n",
            "[01/13/2023-15:26:36] [I] Memory Pools: workspace: default, dlaSRAM: default, dlaLocalDRAM: default, dlaGlobalDRAM: default\n",
            "[01/13/2023-15:26:36] [I] minTiming: 1\n",
            "[01/13/2023-15:26:36] [I] avgTiming: 8\n",
            "[01/13/2023-15:26:36] [I] Precision: FP32\n",
            "[01/13/2023-15:26:36] [I] LayerPrecisions: \n",
            "[01/13/2023-15:26:36] [I] Calibration: \n",
            "[01/13/2023-15:26:36] [I] Refit: Disabled\n",
            "[01/13/2023-15:26:36] [I] Sparsity: Disabled\n",
            "[01/13/2023-15:26:36] [I] Safe mode: Disabled\n",
            "[01/13/2023-15:26:36] [I] DirectIO mode: Disabled\n",
            "[01/13/2023-15:26:36] [I] Restricted mode: Disabled\n",
            "[01/13/2023-15:26:36] [I] Build only: Disabled\n",
            "[01/13/2023-15:26:36] [I] Save engine: \n",
            "[01/13/2023-15:26:36] [I] Load engine: /content/yolov7nms.trt\n",
            "[01/13/2023-15:26:36] [I] Profiling verbosity: 0\n",
            "[01/13/2023-15:26:36] [I] Tactic sources: Using default tactic sources\n",
            "[01/13/2023-15:26:36] [I] timingCacheMode: local\n",
            "[01/13/2023-15:26:36] [I] timingCacheFile: \n",
            "[01/13/2023-15:26:36] [I] Heuristic: Disabled\n",
            "[01/13/2023-15:26:36] [I] Preview Features: Use default preview flags.\n",
            "[01/13/2023-15:26:36] [I] Input(s)s format: fp32:CHW\n",
            "[01/13/2023-15:26:36] [I] Output(s)s format: fp32:CHW\n",
            "[01/13/2023-15:26:36] [I] Input build shapes: model\n",
            "[01/13/2023-15:26:36] [I] Input calibration shapes: model\n",
            "[01/13/2023-15:26:36] [I] === System Options ===\n",
            "[01/13/2023-15:26:36] [I] Device: 0\n",
            "[01/13/2023-15:26:36] [I] DLACore: \n",
            "[01/13/2023-15:26:36] [I] Plugins:\n",
            "[01/13/2023-15:26:36] [I] === Inference Options ===\n",
            "[01/13/2023-15:26:36] [I] Batch: 1\n",
            "[01/13/2023-15:26:36] [I] Input inference shapes: model\n",
            "[01/13/2023-15:26:36] [I] Iterations: 500\n",
            "[01/13/2023-15:26:36] [I] Duration: 1s (+ 500ms warm up)\n",
            "[01/13/2023-15:26:36] [I] Sleep time: 0ms\n",
            "[01/13/2023-15:26:36] [I] Idle time: 0ms\n",
            "[01/13/2023-15:26:36] [I] Streams: 1\n",
            "[01/13/2023-15:26:36] [I] ExposeDMA: Disabled\n",
            "[01/13/2023-15:26:36] [I] Data transfers: Enabled\n",
            "[01/13/2023-15:26:36] [I] Spin-wait: Enabled\n",
            "[01/13/2023-15:26:36] [I] Multithreading: Disabled\n",
            "[01/13/2023-15:26:36] [I] CUDA Graph: Disabled\n",
            "[01/13/2023-15:26:36] [I] Separate profiling: Disabled\n",
            "[01/13/2023-15:26:36] [I] Time Deserialize: Disabled\n",
            "[01/13/2023-15:26:36] [I] Time Refit: Disabled\n",
            "[01/13/2023-15:26:36] [I] NVTX verbosity: 0\n",
            "[01/13/2023-15:26:36] [I] Persistent Cache Ratio: 0\n",
            "[01/13/2023-15:26:36] [I] Inputs:\n",
            "[01/13/2023-15:26:36] [I] === Reporting Options ===\n",
            "[01/13/2023-15:26:36] [I] Verbose: Disabled\n",
            "[01/13/2023-15:26:36] [I] Averages: 10 inferences\n",
            "[01/13/2023-15:26:36] [I] Percentiles: 90,95,99\n",
            "[01/13/2023-15:26:36] [I] Dump refittable layers:Disabled\n",
            "[01/13/2023-15:26:36] [I] Dump output: Disabled\n",
            "[01/13/2023-15:26:36] [I] Profile: Disabled\n",
            "[01/13/2023-15:26:36] [I] Export timing to JSON file: \n",
            "[01/13/2023-15:26:36] [I] Export output to JSON file: \n",
            "[01/13/2023-15:26:36] [I] Export profile to JSON file: \n",
            "[01/13/2023-15:26:36] [I] \n",
            "[01/13/2023-15:26:36] [I] === Device Information ===\n",
            "[01/13/2023-15:26:36] [I] Selected Device: Tesla T4\n",
            "[01/13/2023-15:26:36] [I] Compute Capability: 7.5\n",
            "[01/13/2023-15:26:36] [I] SMs: 40\n",
            "[01/13/2023-15:26:36] [I] Compute Clock Rate: 1.59 GHz\n",
            "[01/13/2023-15:26:36] [I] Device Global Memory: 15109 MiB\n",
            "[01/13/2023-15:26:36] [I] Shared Memory per SM: 64 KiB\n",
            "[01/13/2023-15:26:36] [I] Memory Bus Width: 256 bits (ECC enabled)\n",
            "[01/13/2023-15:26:36] [I] Memory Clock Rate: 5.001 GHz\n",
            "[01/13/2023-15:26:36] [I] \n",
            "[01/13/2023-15:26:36] [I] TensorRT version: 8.5.2\n",
            "[01/13/2023-15:26:36] [I] Engine loaded in 0.0230077 sec.\n",
            "[01/13/2023-15:26:37] [I] [TRT] Loaded engine size: 13 MiB\n",
            "[01/13/2023-15:26:38] [I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +168, now: CPU 0, GPU 1507 (MiB)\n",
            "[01/13/2023-15:26:39] [I] [TRT] [MemUsageChange] Init cuDNN: CPU +0, GPU +172, now: CPU 0, GPU 1679 (MiB)\n",
            "[01/13/2023-15:26:39] [W] [TRT] TensorRT was linked against cuDNN 8.6.0 but loaded cuDNN 8.1.1\n",
            "[01/13/2023-15:26:39] [I] [TRT] [MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +0, GPU +12, now: CPU 0, GPU 12 (MiB)\n",
            "[01/13/2023-15:26:39] [I] Engine deserialized in 2.6418 sec.\n",
            "[01/13/2023-15:26:39] [I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +8, now: CPU 0, GPU 1671 (MiB)\n",
            "[01/13/2023-15:26:39] [I] [TRT] [MemUsageChange] Init cuDNN: CPU +0, GPU +8, now: CPU 0, GPU 1679 (MiB)\n",
            "[01/13/2023-15:26:39] [W] [TRT] TensorRT was linked against cuDNN 8.6.0 but loaded cuDNN 8.1.1\n",
            "[01/13/2023-15:26:39] [I] [TRT] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +51, now: CPU 0, GPU 63 (MiB)\n",
            "[01/13/2023-15:26:39] [W] [TRT] CUDA lazy loading is not enabled. Enabling it can significantly reduce device memory usage. See `CUDA_MODULE_LOADING` in https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#env-vars\n",
            "[01/13/2023-15:26:39] [I] Setting persistentCacheLimit to 0 bytes.\n",
            "[01/13/2023-15:26:39] [I] Using random values for input images\n",
            "[01/13/2023-15:26:39] [I] Created input binding for images with dimensions 1x3x640x640\n",
            "[01/13/2023-15:26:39] [I] Using random values for output num_dets\n",
            "[01/13/2023-15:26:39] [I] Created output binding for num_dets with dimensions 1x1\n",
            "[01/13/2023-15:26:39] [I] Using random values for output det_boxes\n",
            "[01/13/2023-15:26:39] [I] Created output binding for det_boxes with dimensions 1x100x4\n",
            "[01/13/2023-15:26:39] [I] Using random values for output det_scores\n",
            "[01/13/2023-15:26:39] [I] Created output binding for det_scores with dimensions 1x100\n",
            "[01/13/2023-15:26:39] [I] Using random values for output det_classes\n",
            "[01/13/2023-15:26:39] [I] Created output binding for det_classes with dimensions 1x100\n",
            "[01/13/2023-15:26:39] [I] Starting inference\n",
            "[01/13/2023-15:26:40] [I] Warmup completed 156 queries over 500 ms\n",
            "[01/13/2023-15:26:40] [I] Timing trace has 500 queries over 1.10238 s\n",
            "[01/13/2023-15:26:40] [I] \n",
            "[01/13/2023-15:26:40] [I] === Trace details ===\n",
            "[01/13/2023-15:26:40] [I] Trace averages of 10 runs:\n",
            "[01/13/2023-15:26:40] [I] Average on 10 runs - GPU latency: 2.18623 ms - Host latency: 2.64238 ms (enqueue 0.714612 ms)\n",
            "[01/13/2023-15:26:40] [I] Average on 10 runs - GPU latency: 2.18735 ms - Host latency: 2.64576 ms (enqueue 0.69621 ms)\n",
            "[01/13/2023-15:26:40] [I] Average on 10 runs - GPU latency: 2.18781 ms - Host latency: 2.6432 ms (enqueue 0.69729 ms)\n",
            "[01/13/2023-15:26:40] [I] Average on 10 runs - GPU latency: 2.17998 ms - Host latency: 2.63895 ms (enqueue 0.734552 ms)\n",
            "[01/13/2023-15:26:40] [I] Average on 10 runs - GPU latency: 2.17118 ms - Host latency: 2.62867 ms (enqueue 0.732294 ms)\n",
            "[01/13/2023-15:26:40] [I] Average on 10 runs - GPU latency: 2.16807 ms - Host latency: 2.62342 ms (enqueue 0.716089 ms)\n",
            "[01/13/2023-15:26:40] [I] Average on 10 runs - GPU latency: 2.17416 ms - Host latency: 2.62992 ms (enqueue 0.714697 ms)\n",
            "[01/13/2023-15:26:40] [I] Average on 10 runs - GPU latency: 2.16742 ms - Host latency: 2.62516 ms (enqueue 0.708191 ms)\n",
            "[01/13/2023-15:26:40] [I] Average on 10 runs - GPU latency: 2.17993 ms - Host latency: 2.63434 ms (enqueue 0.708124 ms)\n",
            "[01/13/2023-15:26:40] [I] Average on 10 runs - GPU latency: 2.18475 ms - Host latency: 2.63987 ms (enqueue 0.670905 ms)\n",
            "[01/13/2023-15:26:40] [I] Average on 10 runs - GPU latency: 2.18611 ms - Host latency: 2.64278 ms (enqueue 0.657568 ms)\n",
            "[01/13/2023-15:26:40] [I] Average on 10 runs - GPU latency: 2.18543 ms - Host latency: 2.64099 ms (enqueue 0.666437 ms)\n",
            "[01/13/2023-15:26:40] [I] Average on 10 runs - GPU latency: 2.18627 ms - Host latency: 2.63995 ms (enqueue 0.655164 ms)\n",
            "[01/13/2023-15:26:40] [I] Average on 10 runs - GPU latency: 2.16426 ms - Host latency: 2.61796 ms (enqueue 0.656976 ms)\n",
            "[01/13/2023-15:26:40] [I] Average on 10 runs - GPU latency: 2.16108 ms - Host latency: 2.61712 ms (enqueue 0.659393 ms)\n",
            "[01/13/2023-15:26:40] [I] Average on 10 runs - GPU latency: 2.15824 ms - Host latency: 2.61483 ms (enqueue 0.906079 ms)\n",
            "[01/13/2023-15:26:40] [I] Average on 10 runs - GPU latency: 2.16255 ms - Host latency: 2.61753 ms (enqueue 0.662103 ms)\n",
            "[01/13/2023-15:26:40] [I] Average on 10 runs - GPU latency: 2.16194 ms - Host latency: 2.61785 ms (enqueue 0.686749 ms)\n",
            "[01/13/2023-15:26:40] [I] Average on 10 runs - GPU latency: 2.18312 ms - Host latency: 2.6376 ms (enqueue 0.776569 ms)\n",
            "[01/13/2023-15:26:40] [I] Average on 10 runs - GPU latency: 2.20333 ms - Host latency: 2.66129 ms (enqueue 0.84502 ms)\n",
            "[01/13/2023-15:26:40] [I] Average on 10 runs - GPU latency: 2.20728 ms - Host latency: 2.66768 ms (enqueue 0.687256 ms)\n",
            "[01/13/2023-15:26:40] [I] Average on 10 runs - GPU latency: 2.19376 ms - Host latency: 2.65133 ms (enqueue 0.711774 ms)\n",
            "[01/13/2023-15:26:40] [I] Average on 10 runs - GPU latency: 2.19315 ms - Host latency: 2.68265 ms (enqueue 0.699585 ms)\n",
            "[01/13/2023-15:26:40] [I] Average on 10 runs - GPU latency: 2.18856 ms - Host latency: 2.64516 ms (enqueue 0.712183 ms)\n",
            "[01/13/2023-15:26:40] [I] Average on 10 runs - GPU latency: 2.19052 ms - Host latency: 2.64704 ms (enqueue 0.70672 ms)\n",
            "[01/13/2023-15:26:40] [I] Average on 10 runs - GPU latency: 2.18616 ms - Host latency: 2.67769 ms (enqueue 0.700879 ms)\n",
            "[01/13/2023-15:26:40] [I] Average on 10 runs - GPU latency: 2.16228 ms - Host latency: 2.61959 ms (enqueue 0.704834 ms)\n",
            "[01/13/2023-15:26:40] [I] Average on 10 runs - GPU latency: 2.16222 ms - Host latency: 2.62239 ms (enqueue 0.694311 ms)\n",
            "[01/13/2023-15:26:40] [I] Average on 10 runs - GPU latency: 2.16101 ms - Host latency: 2.61548 ms (enqueue 0.683276 ms)\n",
            "[01/13/2023-15:26:40] [I] Average on 10 runs - GPU latency: 2.1608 ms - Host latency: 2.61553 ms (enqueue 0.684351 ms)\n",
            "[01/13/2023-15:26:40] [I] Average on 10 runs - GPU latency: 2.19487 ms - Host latency: 2.65253 ms (enqueue 0.690881 ms)\n",
            "[01/13/2023-15:26:40] [I] Average on 10 runs - GPU latency: 2.25603 ms - Host latency: 2.71299 ms (enqueue 0.713123 ms)\n",
            "[01/13/2023-15:26:40] [I] Average on 10 runs - GPU latency: 2.3129 ms - Host latency: 2.77018 ms (enqueue 0.702649 ms)\n",
            "[01/13/2023-15:26:40] [I] Average on 10 runs - GPU latency: 2.33503 ms - Host latency: 2.83158 ms (enqueue 0.69436 ms)\n",
            "[01/13/2023-15:26:40] [I] Average on 10 runs - GPU latency: 2.33312 ms - Host latency: 2.79349 ms (enqueue 0.709985 ms)\n",
            "[01/13/2023-15:26:40] [I] Average on 10 runs - GPU latency: 2.2952 ms - Host latency: 2.75533 ms (enqueue 0.809338 ms)\n",
            "[01/13/2023-15:26:40] [I] Average on 10 runs - GPU latency: 2.26399 ms - Host latency: 2.71807 ms (enqueue 0.758484 ms)\n",
            "[01/13/2023-15:26:40] [I] Average on 10 runs - GPU latency: 2.23938 ms - Host latency: 2.69261 ms (enqueue 0.757153 ms)\n",
            "[01/13/2023-15:26:40] [I] Average on 10 runs - GPU latency: 2.21589 ms - Host latency: 2.66898 ms (enqueue 0.738379 ms)\n",
            "[01/13/2023-15:26:40] [I] Average on 10 runs - GPU latency: 2.19133 ms - Host latency: 2.6465 ms (enqueue 0.775232 ms)\n",
            "[01/13/2023-15:26:40] [I] Average on 10 runs - GPU latency: 2.19098 ms - Host latency: 2.68375 ms (enqueue 0.739099 ms)\n",
            "[01/13/2023-15:26:40] [I] Average on 10 runs - GPU latency: 2.18765 ms - Host latency: 2.64841 ms (enqueue 0.749683 ms)\n",
            "[01/13/2023-15:26:40] [I] Average on 10 runs - GPU latency: 2.18573 ms - Host latency: 2.6458 ms (enqueue 0.794763 ms)\n",
            "[01/13/2023-15:26:40] [I] Average on 10 runs - GPU latency: 2.19069 ms - Host latency: 2.68553 ms (enqueue 0.909753 ms)\n",
            "[01/13/2023-15:26:40] [I] Average on 10 runs - GPU latency: 2.16619 ms - Host latency: 2.62587 ms (enqueue 0.716431 ms)\n",
            "[01/13/2023-15:26:40] [I] Average on 10 runs - GPU latency: 2.20814 ms - Host latency: 2.66367 ms (enqueue 0.803589 ms)\n",
            "[01/13/2023-15:26:40] [I] Average on 10 runs - GPU latency: 2.20579 ms - Host latency: 2.66171 ms (enqueue 0.780347 ms)\n",
            "[01/13/2023-15:26:40] [I] Average on 10 runs - GPU latency: 2.20544 ms - Host latency: 2.66384 ms (enqueue 0.819336 ms)\n",
            "[01/13/2023-15:26:40] [I] Average on 10 runs - GPU latency: 2.20233 ms - Host latency: 2.6621 ms (enqueue 0.758301 ms)\n",
            "[01/13/2023-15:26:40] [I] Average on 10 runs - GPU latency: 2.18619 ms - Host latency: 2.64786 ms (enqueue 0.71864 ms)\n",
            "[01/13/2023-15:26:40] [I] \n",
            "[01/13/2023-15:26:40] [I] === Performance summary ===\n",
            "[01/13/2023-15:26:40] [I] Throughput: 453.562 qps\n",
            "[01/13/2023-15:26:40] [I] Latency: min = 2.59985 ms, max = 3.16846 ms, mean = 2.6587 ms, median = 2.64386 ms, percentile(90%) = 2.72327 ms, percentile(95%) = 2.7832 ms, percentile(99%) = 2.96631 ms\n",
            "[01/13/2023-15:26:40] [I] Enqueue Time: min = 0.616699 ms, max = 1.31885 ms, mean = 0.727794 ms, median = 0.705078 ms, percentile(90%) = 0.810181 ms, percentile(95%) = 0.893921 ms, percentile(99%) = 1.17511 ms\n",
            "[01/13/2023-15:26:40] [I] H2D Latency: min = 0.436401 ms, max = 0.817993 ms, mean = 0.449507 ms, median = 0.444519 ms, percentile(90%) = 0.4552 ms, percentile(95%) = 0.459106 ms, percentile(99%) = 0.76062 ms\n",
            "[01/13/2023-15:26:40] [I] GPU Compute Time: min = 2.15039 ms, max = 2.35913 ms, mean = 2.19824 ms, median = 2.18726 ms, percentile(90%) = 2.26306 ms, percentile(95%) = 2.32336 ms, percentile(99%) = 2.34033 ms\n",
            "[01/13/2023-15:26:40] [I] D2H Latency: min = 0.0078125 ms, max = 0.02771 ms, mean = 0.010955 ms, median = 0.0108032 ms, percentile(90%) = 0.0119629 ms, percentile(95%) = 0.0125122 ms, percentile(99%) = 0.0168457 ms\n",
            "[01/13/2023-15:26:40] [I] Total Host Walltime: 1.10238 s\n",
            "[01/13/2023-15:26:40] [I] Total GPU Compute Time: 1.09912 s\n",
            "[01/13/2023-15:26:40] [W] * GPU compute time is unstable, with coefficient of variance = 1.97745%.\n",
            "[01/13/2023-15:26:40] [W]   If not already in use, locking GPU clock frequency or adding --useSpinWait may improve the stability.\n",
            "[01/13/2023-15:26:40] [I] Explanations of the performance metrics are printed in the verbose logs.\n",
            "[01/13/2023-15:26:40] [I] \n",
            "&&&& PASSED TensorRT.trtexec [TensorRT v8502] # /usr/src/tensorrt/bin/trtexec --loadEngine=/content/yolov7nms.trt --batch=1 --warmUp=500 --duration=1 --iterations=500 --streams=1 --useSpinWait\n"
          ]
        }
      ],
      "source": [
        "# Run speed test\n",
        "!/usr/src/tensorrt/bin/trtexec --loadEngine=/content/yolov7nms.trt --batch=1 --warmUp=500 --duration=1 --iterations=500 --streams=1 --useSpinWait"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test YOLOv7 without NMS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gp6_ZrG5tMwZ"
      },
      "outputs": [],
      "source": [
        "# Save temp ONN model without NMS layer\n",
        "%cd /content/yolov7/\n",
        "!python export.py --weights ./yolov7-tiny.pt --grid --simplify --topk-all 100 --iou-thres 0.65 --conf-thres 0.35 --img-size 640 640"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N6Ckyme3nGut",
        "outputId": "de1baefe-ba43-4506-883e-e86fcf2f17b0"
      },
      "outputs": [],
      "source": [
        "# Create engine\n",
        "!/usr/src/tensorrt/bin/trtexec --onnx=/content/yolov7/yolov7-tiny.onnx --saveEngine=/content/yolov7.trt --fp16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aq_q1JsDrl3G",
        "outputId": "f4d9bc56-77d3-46e0-e2cd-2d49436c1d36"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "&&&& RUNNING TensorRT.trtexec [TensorRT v8502] # /usr/src/tensorrt/bin/trtexec --loadEngine=/content/yolov7.trt --batch=1 --warmUp=500 --duration=1 --iterations=500 --streams=1 --useSpinWait\n",
            "[01/13/2023-15:42:27] [I] === Model Options ===\n",
            "[01/13/2023-15:42:27] [I] Format: *\n",
            "[01/13/2023-15:42:27] [I] Model: \n",
            "[01/13/2023-15:42:27] [I] Output:\n",
            "[01/13/2023-15:42:27] [I] === Build Options ===\n",
            "[01/13/2023-15:42:27] [I] Max batch: 1\n",
            "[01/13/2023-15:42:27] [I] Memory Pools: workspace: default, dlaSRAM: default, dlaLocalDRAM: default, dlaGlobalDRAM: default\n",
            "[01/13/2023-15:42:27] [I] minTiming: 1\n",
            "[01/13/2023-15:42:27] [I] avgTiming: 8\n",
            "[01/13/2023-15:42:27] [I] Precision: FP32\n",
            "[01/13/2023-15:42:27] [I] LayerPrecisions: \n",
            "[01/13/2023-15:42:27] [I] Calibration: \n",
            "[01/13/2023-15:42:27] [I] Refit: Disabled\n",
            "[01/13/2023-15:42:27] [I] Sparsity: Disabled\n",
            "[01/13/2023-15:42:27] [I] Safe mode: Disabled\n",
            "[01/13/2023-15:42:27] [I] DirectIO mode: Disabled\n",
            "[01/13/2023-15:42:27] [I] Restricted mode: Disabled\n",
            "[01/13/2023-15:42:27] [I] Build only: Disabled\n",
            "[01/13/2023-15:42:27] [I] Save engine: \n",
            "[01/13/2023-15:42:27] [I] Load engine: /content/yolov7.trt\n",
            "[01/13/2023-15:42:27] [I] Profiling verbosity: 0\n",
            "[01/13/2023-15:42:27] [I] Tactic sources: Using default tactic sources\n",
            "[01/13/2023-15:42:27] [I] timingCacheMode: local\n",
            "[01/13/2023-15:42:27] [I] timingCacheFile: \n",
            "[01/13/2023-15:42:27] [I] Heuristic: Disabled\n",
            "[01/13/2023-15:42:27] [I] Preview Features: Use default preview flags.\n",
            "[01/13/2023-15:42:27] [I] Input(s)s format: fp32:CHW\n",
            "[01/13/2023-15:42:27] [I] Output(s)s format: fp32:CHW\n",
            "[01/13/2023-15:42:27] [I] Input build shapes: model\n",
            "[01/13/2023-15:42:27] [I] Input calibration shapes: model\n",
            "[01/13/2023-15:42:27] [I] === System Options ===\n",
            "[01/13/2023-15:42:27] [I] Device: 0\n",
            "[01/13/2023-15:42:27] [I] DLACore: \n",
            "[01/13/2023-15:42:27] [I] Plugins:\n",
            "[01/13/2023-15:42:27] [I] === Inference Options ===\n",
            "[01/13/2023-15:42:27] [I] Batch: 1\n",
            "[01/13/2023-15:42:27] [I] Input inference shapes: model\n",
            "[01/13/2023-15:42:27] [I] Iterations: 500\n",
            "[01/13/2023-15:42:27] [I] Duration: 1s (+ 500ms warm up)\n",
            "[01/13/2023-15:42:27] [I] Sleep time: 0ms\n",
            "[01/13/2023-15:42:27] [I] Idle time: 0ms\n",
            "[01/13/2023-15:42:27] [I] Streams: 1\n",
            "[01/13/2023-15:42:27] [I] ExposeDMA: Disabled\n",
            "[01/13/2023-15:42:27] [I] Data transfers: Enabled\n",
            "[01/13/2023-15:42:27] [I] Spin-wait: Enabled\n",
            "[01/13/2023-15:42:27] [I] Multithreading: Disabled\n",
            "[01/13/2023-15:42:27] [I] CUDA Graph: Disabled\n",
            "[01/13/2023-15:42:27] [I] Separate profiling: Disabled\n",
            "[01/13/2023-15:42:27] [I] Time Deserialize: Disabled\n",
            "[01/13/2023-15:42:27] [I] Time Refit: Disabled\n",
            "[01/13/2023-15:42:27] [I] NVTX verbosity: 0\n",
            "[01/13/2023-15:42:27] [I] Persistent Cache Ratio: 0\n",
            "[01/13/2023-15:42:27] [I] Inputs:\n",
            "[01/13/2023-15:42:27] [I] === Reporting Options ===\n",
            "[01/13/2023-15:42:27] [I] Verbose: Disabled\n",
            "[01/13/2023-15:42:27] [I] Averages: 10 inferences\n",
            "[01/13/2023-15:42:27] [I] Percentiles: 90,95,99\n",
            "[01/13/2023-15:42:27] [I] Dump refittable layers:Disabled\n",
            "[01/13/2023-15:42:27] [I] Dump output: Disabled\n",
            "[01/13/2023-15:42:27] [I] Profile: Disabled\n",
            "[01/13/2023-15:42:27] [I] Export timing to JSON file: \n",
            "[01/13/2023-15:42:27] [I] Export output to JSON file: \n",
            "[01/13/2023-15:42:27] [I] Export profile to JSON file: \n",
            "[01/13/2023-15:42:27] [I] \n",
            "[01/13/2023-15:42:27] [I] === Device Information ===\n",
            "[01/13/2023-15:42:27] [I] Selected Device: Tesla T4\n",
            "[01/13/2023-15:42:27] [I] Compute Capability: 7.5\n",
            "[01/13/2023-15:42:27] [I] SMs: 40\n",
            "[01/13/2023-15:42:27] [I] Compute Clock Rate: 1.59 GHz\n",
            "[01/13/2023-15:42:27] [I] Device Global Memory: 15109 MiB\n",
            "[01/13/2023-15:42:27] [I] Shared Memory per SM: 64 KiB\n",
            "[01/13/2023-15:42:27] [I] Memory Bus Width: 256 bits (ECC enabled)\n",
            "[01/13/2023-15:42:27] [I] Memory Clock Rate: 5.001 GHz\n",
            "[01/13/2023-15:42:27] [I] \n",
            "[01/13/2023-15:42:27] [I] TensorRT version: 8.5.2\n",
            "[01/13/2023-15:42:27] [I] Engine loaded in 0.0209839 sec.\n",
            "[01/13/2023-15:42:28] [I] [TRT] Loaded engine size: 13 MiB\n",
            "[01/13/2023-15:42:29] [I] [TRT] [MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +0, GPU +12, now: CPU 0, GPU 12 (MiB)\n",
            "[01/13/2023-15:42:29] [I] Engine deserialized in 1.51958 sec.\n",
            "[01/13/2023-15:42:29] [I] [TRT] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +19, now: CPU 0, GPU 31 (MiB)\n",
            "[01/13/2023-15:42:29] [W] [TRT] CUDA lazy loading is not enabled. Enabling it can significantly reduce device memory usage. See `CUDA_MODULE_LOADING` in https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#env-vars\n",
            "[01/13/2023-15:42:29] [I] Setting persistentCacheLimit to 0 bytes.\n",
            "[01/13/2023-15:42:29] [I] Using random values for input images\n",
            "[01/13/2023-15:42:29] [I] Created input binding for images with dimensions 1x3x640x640\n",
            "[01/13/2023-15:42:29] [I] Using random values for output output\n",
            "[01/13/2023-15:42:29] [I] Created output binding for output with dimensions 1x25200x85\n",
            "[01/13/2023-15:42:29] [I] Starting inference\n",
            "[01/13/2023-15:42:31] [I] Warmup completed 176 queries over 500 ms\n",
            "[01/13/2023-15:42:31] [I] Timing trace has 500 queries over 1.10009 s\n",
            "[01/13/2023-15:42:31] [I] \n",
            "[01/13/2023-15:42:31] [I] === Trace details ===\n",
            "[01/13/2023-15:42:31] [I] Trace averages of 10 runs:\n",
            "[01/13/2023-15:42:31] [I] Average on 10 runs - GPU latency: 2.17117 ms - Host latency: 3.52715 ms (enqueue 0.637054 ms)\n",
            "[01/13/2023-15:42:31] [I] Average on 10 runs - GPU latency: 2.19403 ms - Host latency: 3.5521 ms (enqueue 0.647876 ms)\n",
            "[01/13/2023-15:42:31] [I] Average on 10 runs - GPU latency: 2.19531 ms - Host latency: 3.55074 ms (enqueue 0.645459 ms)\n",
            "[01/13/2023-15:42:31] [I] Average on 10 runs - GPU latency: 2.19412 ms - Host latency: 3.54592 ms (enqueue 0.64577 ms)\n",
            "[01/13/2023-15:42:31] [I] Average on 10 runs - GPU latency: 2.18018 ms - Host latency: 3.54175 ms (enqueue 0.7039 ms)\n",
            "[01/13/2023-15:42:31] [I] Average on 10 runs - GPU latency: 2.17178 ms - Host latency: 3.52708 ms (enqueue 0.637128 ms)\n",
            "[01/13/2023-15:42:31] [I] Average on 10 runs - GPU latency: 2.17897 ms - Host latency: 3.53309 ms (enqueue 0.651459 ms)\n",
            "[01/13/2023-15:42:31] [I] Average on 10 runs - GPU latency: 2.17612 ms - Host latency: 3.52735 ms (enqueue 0.628931 ms)\n",
            "[01/13/2023-15:42:31] [I] Average on 10 runs - GPU latency: 2.16822 ms - Host latency: 3.52289 ms (enqueue 0.650189 ms)\n",
            "[01/13/2023-15:42:31] [I] Average on 10 runs - GPU latency: 2.15275 ms - Host latency: 3.51315 ms (enqueue 0.649432 ms)\n",
            "[01/13/2023-15:42:31] [I] Average on 10 runs - GPU latency: 2.15782 ms - Host latency: 3.51889 ms (enqueue 0.644495 ms)\n",
            "[01/13/2023-15:42:31] [I] Average on 10 runs - GPU latency: 2.16967 ms - Host latency: 3.54954 ms (enqueue 0.743414 ms)\n",
            "[01/13/2023-15:42:31] [I] Average on 10 runs - GPU latency: 2.19219 ms - Host latency: 3.5481 ms (enqueue 0.654327 ms)\n",
            "[01/13/2023-15:42:31] [I] Average on 10 runs - GPU latency: 2.20263 ms - Host latency: 3.57729 ms (enqueue 0.686951 ms)\n",
            "[01/13/2023-15:42:31] [I] Average on 10 runs - GPU latency: 2.19434 ms - Host latency: 3.58278 ms (enqueue 0.637738 ms)\n",
            "[01/13/2023-15:42:31] [I] Average on 10 runs - GPU latency: 2.18199 ms - Host latency: 3.55533 ms (enqueue 0.655286 ms)\n",
            "[01/13/2023-15:42:31] [I] Average on 10 runs - GPU latency: 2.17599 ms - Host latency: 3.5299 ms (enqueue 0.643121 ms)\n",
            "[01/13/2023-15:42:31] [I] Average on 10 runs - GPU latency: 2.17975 ms - Host latency: 3.53658 ms (enqueue 0.624445 ms)\n",
            "[01/13/2023-15:42:31] [I] Average on 10 runs - GPU latency: 2.17346 ms - Host latency: 3.52598 ms (enqueue 0.624548 ms)\n",
            "[01/13/2023-15:42:31] [I] Average on 10 runs - GPU latency: 2.17443 ms - Host latency: 3.53303 ms (enqueue 0.65423 ms)\n",
            "[01/13/2023-15:42:31] [I] Average on 10 runs - GPU latency: 2.16376 ms - Host latency: 3.54493 ms (enqueue 0.667084 ms)\n",
            "[01/13/2023-15:42:31] [I] Average on 10 runs - GPU latency: 2.15875 ms - Host latency: 3.52286 ms (enqueue 0.635291 ms)\n",
            "[01/13/2023-15:42:31] [I] Average on 10 runs - GPU latency: 2.19017 ms - Host latency: 3.55042 ms (enqueue 0.868744 ms)\n",
            "[01/13/2023-15:42:31] [I] Average on 10 runs - GPU latency: 2.19224 ms - Host latency: 3.5561 ms (enqueue 0.709216 ms)\n",
            "[01/13/2023-15:42:31] [I] Average on 10 runs - GPU latency: 2.19033 ms - Host latency: 3.54348 ms (enqueue 0.65907 ms)\n",
            "[01/13/2023-15:42:31] [I] Average on 10 runs - GPU latency: 2.18339 ms - Host latency: 3.53461 ms (enqueue 0.658875 ms)\n",
            "[01/13/2023-15:42:31] [I] Average on 10 runs - GPU latency: 2.17476 ms - Host latency: 3.53196 ms (enqueue 0.636401 ms)\n",
            "[01/13/2023-15:42:31] [I] Average on 10 runs - GPU latency: 2.17644 ms - Host latency: 3.53003 ms (enqueue 0.656482 ms)\n",
            "[01/13/2023-15:42:31] [I] Average on 10 runs - GPU latency: 2.17501 ms - Host latency: 3.52648 ms (enqueue 0.641919 ms)\n",
            "[01/13/2023-15:42:31] [I] Average on 10 runs - GPU latency: 2.19075 ms - Host latency: 3.5443 ms (enqueue 0.700195 ms)\n",
            "[01/13/2023-15:42:31] [I] Average on 10 runs - GPU latency: 2.23824 ms - Host latency: 3.59614 ms (enqueue 0.65719 ms)\n",
            "[01/13/2023-15:42:31] [I] Average on 10 runs - GPU latency: 2.29315 ms - Host latency: 3.67744 ms (enqueue 0.658569 ms)\n",
            "[01/13/2023-15:42:31] [I] Average on 10 runs - GPU latency: 2.2916 ms - Host latency: 3.65072 ms (enqueue 0.66477 ms)\n",
            "[01/13/2023-15:42:31] [I] Average on 10 runs - GPU latency: 2.25371 ms - Host latency: 3.60839 ms (enqueue 0.670508 ms)\n",
            "[01/13/2023-15:42:31] [I] Average on 10 runs - GPU latency: 2.25121 ms - Host latency: 3.63364 ms (enqueue 0.774036 ms)\n",
            "[01/13/2023-15:42:31] [I] Average on 10 runs - GPU latency: 2.23802 ms - Host latency: 3.59056 ms (enqueue 0.65387 ms)\n",
            "[01/13/2023-15:42:31] [I] Average on 10 runs - GPU latency: 2.21658 ms - Host latency: 3.56875 ms (enqueue 0.694409 ms)\n",
            "[01/13/2023-15:42:31] [I] Average on 10 runs - GPU latency: 2.19413 ms - Host latency: 3.55048 ms (enqueue 0.671374 ms)\n",
            "[01/13/2023-15:42:31] [I] Average on 10 runs - GPU latency: 2.19257 ms - Host latency: 3.55221 ms (enqueue 0.718176 ms)\n",
            "[01/13/2023-15:42:31] [I] Average on 10 runs - GPU latency: 2.19392 ms - Host latency: 3.58024 ms (enqueue 0.95575 ms)\n",
            "[01/13/2023-15:42:31] [I] Average on 10 runs - GPU latency: 2.19164 ms - Host latency: 3.54656 ms (enqueue 0.658203 ms)\n",
            "[01/13/2023-15:42:31] [I] Average on 10 runs - GPU latency: 2.18812 ms - Host latency: 3.53904 ms (enqueue 0.620593 ms)\n",
            "[01/13/2023-15:42:31] [I] Average on 10 runs - GPU latency: 2.17205 ms - Host latency: 3.52787 ms (enqueue 0.648169 ms)\n",
            "[01/13/2023-15:42:31] [I] Average on 10 runs - GPU latency: 2.1788 ms - Host latency: 3.53557 ms (enqueue 0.678711 ms)\n",
            "[01/13/2023-15:42:31] [I] Average on 10 runs - GPU latency: 2.21263 ms - Host latency: 3.57207 ms (enqueue 0.656567 ms)\n",
            "[01/13/2023-15:42:31] [I] Average on 10 runs - GPU latency: 2.21427 ms - Host latency: 3.60455 ms (enqueue 0.722424 ms)\n",
            "[01/13/2023-15:42:31] [I] Average on 10 runs - GPU latency: 2.21035 ms - Host latency: 3.57102 ms (enqueue 0.81228 ms)\n",
            "[01/13/2023-15:42:31] [I] Average on 10 runs - GPU latency: 2.19655 ms - Host latency: 3.5514 ms (enqueue 0.649353 ms)\n",
            "[01/13/2023-15:42:31] [I] Average on 10 runs - GPU latency: 2.18875 ms - Host latency: 3.53551 ms (enqueue 0.606628 ms)\n",
            "[01/13/2023-15:42:31] [I] Average on 10 runs - GPU latency: 2.18612 ms - Host latency: 3.5343 ms (enqueue 0.659692 ms)\n",
            "[01/13/2023-15:42:31] [I] \n",
            "[01/13/2023-15:42:31] [I] === Performance summary ===\n",
            "[01/13/2023-15:42:31] [I] Throughput: 454.508 qps\n",
            "[01/13/2023-15:42:31] [I] Latency: min = 3.47937 ms, max = 3.94958 ms, mean = 3.55421 ms, median = 3.54456 ms, percentile(90%) = 3.60291 ms, percentile(95%) = 3.63037 ms, percentile(99%) = 3.82068 ms\n",
            "[01/13/2023-15:42:31] [I] Enqueue Time: min = 0.58667 ms, max = 1.11743 ms, mean = 0.674606 ms, median = 0.647156 ms, percentile(90%) = 0.752197 ms, percentile(95%) = 0.932617 ms, percentile(99%) = 1.04425 ms\n",
            "[01/13/2023-15:42:31] [I] H2D Latency: min = 0.516479 ms, max = 0.828918 ms, mean = 0.536091 ms, median = 0.530426 ms, percentile(90%) = 0.542236 ms, percentile(95%) = 0.549316 ms, percentile(99%) = 0.822632 ms\n",
            "[01/13/2023-15:42:31] [I] GPU Compute Time: min = 2.13992 ms, max = 2.32239 ms, mean = 2.19366 ms, median = 2.18768 ms, percentile(90%) = 2.24048 ms, percentile(95%) = 2.25671 ms, percentile(99%) = 2.30127 ms\n",
            "[01/13/2023-15:42:31] [I] D2H Latency: min = 0.811157 ms, max = 0.854126 ms, mean = 0.824456 ms, median = 0.824097 ms, percentile(90%) = 0.830688 ms, percentile(95%) = 0.833496 ms, percentile(99%) = 0.841064 ms\n",
            "[01/13/2023-15:42:31] [I] Total Host Walltime: 1.10009 s\n",
            "[01/13/2023-15:42:31] [I] Total GPU Compute Time: 1.09683 s\n",
            "[01/13/2023-15:42:31] [W] * GPU compute time is unstable, with coefficient of variance = 1.46075%.\n",
            "[01/13/2023-15:42:31] [W]   If not already in use, locking GPU clock frequency or adding --useSpinWait may improve the stability.\n",
            "[01/13/2023-15:42:31] [I] Explanations of the performance metrics are printed in the verbose logs.\n",
            "[01/13/2023-15:42:31] [I] \n",
            "&&&& PASSED TensorRT.trtexec [TensorRT v8502] # /usr/src/tensorrt/bin/trtexec --loadEngine=/content/yolov7.trt --batch=1 --warmUp=500 --duration=1 --iterations=500 --streams=1 --useSpinWait\n"
          ]
        }
      ],
      "source": [
        "# Run speed test\n",
        "!/usr/src/tensorrt/bin/trtexec --loadEngine=/content/yolov7.trt --batch=1 --warmUp=500 --duration=1 --iterations=500 --streams=1 --useSpinWait"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Results\n",
        "Math bigger host to device time in YOLO without NMS doesn't compensate additional GPU time in version with NMS. Transfer all output tensor is very expensive in compare with only bounding boxes.  \n",
        "\n",
        "**With NMS:**\n",
        "- Latency: mean = **2.6587** ms\n",
        "- GPU Compute Time: mean = 2.19824 ms\n",
        "- D2H Latency: min = mean = 0.010955 ms\n",
        "\n",
        "**Withiot NMS:**\n",
        "- Latency: mean = **3.55421** ms\n",
        "- GPU Compute Time: mean = 2.19366 ms\n",
        "- D2H Latency: min = mean = 0.824456 ms"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.6 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
